{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "linear_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwd38XvZK2sy"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6ewR7_sHJ00",
        "outputId": "e9d847eb-a5be-4453-d109-5c9e3b07da70"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn2s4LVCHL0J",
        "outputId": "6dfe675d-1707-4e47-f469-7ccf483b773f"
      },
      "source": [
        "!pip install --upgrade git+git://github.com/frehseg/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/frehseg/nvcc4jupyter.git\n",
            "  Cloning git://github.com/frehseg/nvcc4jupyter.git to /tmp/pip-req-build-_jtw493i\n",
            "  Running command git clone -q git://github.com/frehseg/nvcc4jupyter.git /tmp/pip-req-build-_jtw493i\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.1-cp36-none-any.whl size=2095 sha256=3da9d0b8c49834ec62bf5ec9289cb214d4eadeaf3110595c7c0d3981ec8b7eba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ipkqlm41/wheels/a4/a5/24/17a2b61f9a725a10155cc6fca753aae28436921df21fa16114\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoKiUawdHMGn"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUIyq5vPjxKc"
      },
      "source": [
        "Based on the lecture:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cweMlOB0L4mG"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-TEvrWMJ_a"
      },
      "source": [
        "## CUDA Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-lgwhE1N5_7",
        "outputId": "1a61c826-b8a4-480a-e136-8eb4cf549baf"
      },
      "source": [
        "%%writefile cuda_stuff.cuh\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#ifndef cuda_stuff_H\n",
        "#define cuda_stuff_H\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   nb_rows = number of rows */\n",
        "#define IDX2C(i,j,nb_rows) (((j)*(nb_rows))+(i))\n",
        " \n",
        "//MACRO TO DEBUGG CUDA FUNCTIONS\n",
        "/** Error checking,\n",
        " *  taken from https://stackoverflow.com/questions/14038589/what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess) \n",
        "   {\n",
        "      fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) exit(code);\n",
        "   }\n",
        "}\n",
        "/** Error checking for use with CUDA Dynamic Parallelism */\n",
        "/*\n",
        "#define cdpErrchk(ans) { cdpAssert((ans), __FILE__, __LINE__); }\n",
        "__device__ void cdpAssert(cudaError_t code, const char *file, int line, bool abort=true)\n",
        "{\n",
        "   if (code != cudaSuccess)\n",
        "   {\n",
        "      printf(\"GPU kernel assert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "      if (abort) assert(0);\n",
        "   }\n",
        "}\n",
        "*/\n",
        "void device_synchronize();\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cuda_stuff.cuh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iivrxLaYOYPh",
        "outputId": "95caa413-7e58-4f59-f770-f4d171f1aa50"
      },
      "source": [
        "%%writefile cuda_stuff.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "\n",
        "void device_synchronize(){\n",
        "    gpuErrchk(cudaDeviceSynchronize());\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing cuda_stuff.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsEMpauK8lW"
      },
      "source": [
        "## fmatrix Matrix Tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A97U902HMog4",
        "outputId": "9c318b0e-90fa-4546-c0f9-53e78ef789c0"
      },
      "source": [
        "%%writefile fmatrix.cuh\n",
        "#ifndef fmatrices_H\n",
        "#define fmatrices_H\n",
        "#include \"cuda_stuff.cuh\" // for IDX2C\n",
        "\n",
        "typedef struct {\n",
        "    float* data;\n",
        "    int cols;\n",
        "    int rows;\n",
        "} fmatrix;\n",
        "\n",
        "/* Access element (i,j) of matrix mat */\n",
        "#define getfm(mat,i,j) (mat.data[IDX2C(i,j,mat.rows)])\n",
        "\n",
        "\n",
        "int fmatrix_elements(fmatrix mat);\n",
        "int fmatrix_size(fmatrix mat);\n",
        "/** Assert that the matrix is coherent: all fields nonzero. */\n",
        "void fmatrix_assert();\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols);\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols);\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device);\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device);\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_host);\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host);\n",
        "void fmatrix_free_on_host(fmatrix* mat);\n",
        "void fmatrix_free_on_device(fmatrix* mat);\n",
        "\n",
        "/** Create a matrix representing columns [a,b) of M. \n",
        " *  Note that the new matrix points into the\n",
        " *  data of M. The data is not copied to a new location.\n",
        " */\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the host. \n",
        " *  If nb<0, print all rows. \n",
        " */\n",
        " __host__\n",
        " __device__\n",
        "void fmatrix_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "/** Print the first nb rows of the matrix mat\n",
        " *  on the device. \n",
        " *  If nb<0, print all rows. \n",
        " *\n",
        " *  This version copies the matrix to host first.\n",
        " */\n",
        "void fmatrix_device_print(fmatrix mat, int nb=-1);\n",
        "\n",
        "#endif\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing fmatrix.cuh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGwZ36ifWQ-d",
        "outputId": "6413085c-0a0a-4e99-be00-c9d9b6dacd53"
      },
      "source": [
        "%%writefile fmatrix.cu\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "int fmatrix_elements(fmatrix mat) {\n",
        "     return mat.cols*mat.rows;\n",
        "}\n",
        "\n",
        "int fmatrix_size(fmatrix mat) {\n",
        "     return fmatrix_elements(mat) * sizeof(mat.data[0]);\n",
        "}\n",
        "\n",
        "void fmatrix_assert(fmatrix mat) {\n",
        "    assert(mat.data);\n",
        "    assert(mat.cols);\n",
        "    assert(mat.rows);\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_host(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    mat.data = (float*)malloc(fmatrix_size(mat)); \n",
        "    assert(mat.data);\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_create_on_device(int rows, int cols) {\n",
        "    assert(cols>0);\n",
        "    assert(rows>0);\n",
        "    fmatrix mat;\n",
        "    mat.cols = cols;\n",
        "    mat.rows = rows;\n",
        "    gpuErrchk( \n",
        "        cudaMalloc((void **)&(mat.data), fmatrix_size(mat)) \n",
        "    );\n",
        "    return mat;\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_device(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk( \n",
        "        cudaMemcpy( mat_device.data, mat_host.data, \n",
        "                   fmatrix_size(mat_host), \n",
        "                   cudaMemcpyHostToDevice \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "void fmatrix_data_to_host(fmatrix mat_host, fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix_assert(mat_device);\n",
        "    assert(mat_host.cols==mat_device.cols);\n",
        "    assert(mat_host.rows==mat_device.rows);\n",
        "    gpuErrchk(\n",
        "        cudaMemcpy( mat_host.data, mat_device.data,  \n",
        "                   fmatrix_size(mat_device), \n",
        "                   cudaMemcpyDeviceToHost \n",
        "                   )\n",
        "        );\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_host(fmatrix mat_device) {\n",
        "    fmatrix_assert(mat_device);\n",
        "    fmatrix mat_host = fmatrix_create_on_host(mat_device.rows, mat_device.cols);\n",
        "    fmatrix_data_to_host(mat_host,mat_device);\n",
        "    return mat_host;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_copy_to_device(fmatrix mat_host) {\n",
        "    fmatrix_assert(mat_host);\n",
        "    fmatrix mat_device = fmatrix_create_on_device(mat_host.rows, mat_host.cols);\n",
        "    fmatrix_data_to_device(mat_host,mat_device);\n",
        "    return mat_device;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_host(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  free(mat->data);\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "void fmatrix_free_on_device(fmatrix* mat) {\n",
        "    fmatrix_assert(*mat);  \n",
        "  gpuErrchk(cudaFree(mat->data));\n",
        "  mat->data = 0;\n",
        "  mat->cols = 0;\n",
        "  mat->rows = 0;\n",
        "}\n",
        "\n",
        "fmatrix fmatrix_subcolumns(fmatrix M, int a, int b) {\n",
        "    fmatrix_assert(M);  \n",
        "    fmatrix A = { \n",
        "        .data = &getfm(M,0,a),  \n",
        "        .cols = b-a,\n",
        "        .rows = M.rows \n",
        "    };\n",
        "    fmatrix_assert(A);  \n",
        "    return A;\n",
        "}\n",
        "\n",
        "\n",
        "__host__\n",
        "__device__\n",
        "void fmatrix_print(fmatrix mat, int nb){\n",
        "    if (nb<0 || nb > mat.rows) {\n",
        "        nb = mat.rows;\n",
        "    }\n",
        "    printf(\"[\\n\");\n",
        "    for (int i = 0 ; i < nb; i++){\n",
        "      for (int j = 0 ; j<mat.cols; j++){\n",
        "        printf(\"%f\", getfm(mat,i,j));\n",
        "        if (j+1<mat.cols) {\n",
        "          printf(\",\\t\");\n",
        "        }\n",
        "      }\n",
        "      if (i+1<nb) {\n",
        "        printf(\";\\n\");\n",
        "      }\n",
        "    }\n",
        "    if (nb < mat.rows) {\n",
        "      printf(\"\\n...\\n\");\n",
        "    }\n",
        "  printf(\"\\n]\\n\");\n",
        "}\n",
        "\n",
        "void fmatrix_device_print(fmatrix mat, int nb){\n",
        "   // allocate copy\n",
        "   fmatrix tmp = fmatrix_copy_to_host(mat);\n",
        "   fmatrix_print(tmp,nb);\n",
        "   fmatrix_free_on_host(&tmp);\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing fmatrix.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsKLTK8ELOdN"
      },
      "source": [
        "## Data I/O"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD7rmOBmWfsC",
        "outputId": "3abe2ec5-d50c-44b1-819c-541ac53ef9b4"
      },
      "source": [
        "%%writefile read_csv.cuh\n",
        "#include <cuda_runtime.h>\n",
        "#ifndef read_csv_H\n",
        "#define read_csv_H\n",
        "\n",
        "void read_csv(const char* filename, float* data_array,int nbrow,int nbcol);\n",
        "\n",
        "#endif"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing read_csv.cuh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeedFsZ_WQx0",
        "outputId": "160df1cf-ce55-494e-f460-2c16e0da652d"
      },
      "source": [
        "%%writefile read_csv.cu\n",
        "\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <fstream>\n",
        "\n",
        "#include \"read_csv.cuh\"\n",
        "#include \"cuda_stuff.cuh\" // for matrix indexing\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Functions for reading the dataset from a file\n",
        "/////////////////////////////////////////////////////////\n",
        "\n",
        "/* Read a csv file with a given number of rows and columns */\n",
        "void read_csv(const char* filename, float* data_array,int nbrow,int nbcol) {\n",
        "  string row_as_string;\n",
        "  string value;\n",
        "  double ioTemp;\n",
        "  ifstream infile;\n",
        "  infile.open(filename, ifstream::in);\n",
        "  int row_count = 0;\n",
        "\tif (infile.is_open())\n",
        "  {\n",
        "      // read the headers (and discard)\n",
        "\t\t\tgetline(infile, row_as_string, '\\n');\n",
        "      cout << \"headers: \" << row_as_string << \"!\" << std::endl;\n",
        "      for(int i = 0; i < nbrow; i++){\n",
        "  \t\t\tgetline(infile, row_as_string, '\\n');\n",
        "        // cout << \"read line \" << row_as_string << \"!\" << std::endl;\n",
        "\t\t\t\tistringstream line_stream(row_as_string);\n",
        "\t\t\t  for(int j = 0; j < nbcol; j++){\n",
        "          getline(line_stream, value, ',');\n",
        "\t\t\t\t\tioTemp = strtod(value.c_str(), NULL); \n",
        "          // cout << \"(\"<<i<<\",\"<<j<<\") = \"<< ioTemp << std::endl;\n",
        "\n",
        "\t\t\t\t\tdata_array[IDX2C(i,j,nbrow)] = ioTemp;\n",
        "\n",
        "\t\t\t\t}\n",
        "        ++row_count;\n",
        "\t\t\t}\n",
        "\t\tinfile.close();\n",
        "    cout << \"Read \" << row_count << \" rows.\" << std::endl;\n",
        "\t}\n",
        "\telse cout << \"Cannot open file.\" << endl;\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing read_csv.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex8ilQdYYroU",
        "outputId": "28d6b4da-7dd7-44f5-f1b7-6df30a533d5c"
      },
      "source": [
        "%%writefile preprocess_data.cuh\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#ifndef preprocess_data_H\n",
        "#define preprocess_data_H\n",
        "\n",
        "void get_inputs_and_labels(float* data_array, float** input_array, float** label_array, int nbrows, int nbcols, int nb_inputs, int nb_labels );\n",
        "\n",
        "#endif"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing preprocess_data.cuh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaeUdw_KYaCx",
        "outputId": "829ce0e3-d405-471d-9985-820dfc9a395d"
      },
      "source": [
        "%%writefile preprocess_data.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <fstream>\n",
        "\n",
        "/*Matrix multiplication functions and other auxiliary functions*/\n",
        "#include \"preprocess_data.cuh\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/* transform matrix index to vector offset\n",
        "   Since CUDA uses column major, \n",
        "   ld = number of rows \n",
        "   Example of use: a[IDX2C(0, 1, 50)] */\n",
        "#define IDX2C(i,j,ld) (((j)*(ld))+(i))\n",
        "\n",
        "//Number of thread per block\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "/* Constants for housing data set */\n",
        "#define data_columns  (9)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Number of rows in arrays to print for debugging\n",
        "/////////////////////////////////////////////////////////\n",
        "#define print_rows (10)\n",
        "/////////////////////////////////////////////////////////\n",
        "// Functions for preprocessing the data set\n",
        "/////////////////////////////////////////////////////////\n",
        "\n",
        "/* Split data into inputs and labels. Allocated memory for inputs and labels.\n",
        "   Since cuBLAS is column major, each input is in a column.\n",
        "   We also add 1.0 as first element to each input vector.\n",
        "*/\n",
        "void get_inputs_and_labels(float* data_array, float** input_array, float** label_array, int nbrows, int nbcols, int nb_inputs, int nb_labels ) {\n",
        "    // The inputs are the first nbrows-1 columns.\n",
        "    // The labels are the last column (index nbrows-1), booleanized\n",
        "    // by the condition >= above_threshold\n",
        "    *input_array = (float *)malloc(nbrows * nb_inputs * sizeof(float));    \n",
        "    *label_array = (float *)malloc(nbrows * nb_labels * sizeof(float));    \n",
        "    //cout << &input_array << \" and \"<< &label_array << \" data \" << data_array << std::endl;\n",
        "    cout << \"Allocated memory for inputs: \" << nbrows << \" rows, \"<< nb_inputs << \" columns.\" << std::endl;\n",
        "    cout << \"Allocated memory for labels: \" << nbrows << \" rows, \"<< nb_labels << \" columns.\" << std::endl;\n",
        "\n",
        "    // Copy the data to X\n",
        "    for(int i = 0; i < nbrows; i++){\n",
        "      // Set the first element of each x to 1  \n",
        "      (*input_array)[IDX2C(0,i,nb_inputs)] = 1.0;\n",
        "      // Copy the rest of x\n",
        "\t\t\tfor(int j = 1; j < nb_inputs; j++){\n",
        "\t\t\t\t(*input_array)[IDX2C(j,i,nb_inputs)] = data_array[IDX2C(i,j-1,nbrows)];\n",
        "\t\t\t}\n",
        "      float median_house_value = data_array[IDX2C(i,nbcols-1,nbrows)];\n",
        "      (*label_array)[IDX2C(0,i,nb_labels)] = 0.0;\n",
        "      (*label_array)[IDX2C(1,i,nb_labels)] = 0.0;\n",
        "      if (median_house_value >= above_threshold) {\n",
        "        (*label_array)[IDX2C(0,i,nb_labels)] = 1.0;\n",
        "      } else {\n",
        "        (*label_array)[IDX2C(1,i,nb_labels)] = 1.0;        \n",
        "      }\n",
        "\t\t}   \n",
        "\n",
        "    \n",
        "    // Show some entries for double checking\n",
        "    cout << \"Inputs (first \"<<print_rows<<\"):\" << std::endl;\n",
        "\t  for(int j = 0; j < nb_inputs; j++){\n",
        "      for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\t\tcout << (*input_array)[IDX2C(j,i,nb_inputs)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "    cout << \"Labels (first \"<<print_rows<<\"):\" << std::endl;\n",
        "    for(int j = 0; j < nb_labels; j++){\n",
        "      for(int i = 0; i < nbrows && i < print_rows; i++){\n",
        "\t\t\t\tcout << (*label_array)[IDX2C(j,i,nb_labels)] << \"\\t\";\n",
        "\t\t\t}\n",
        "      cout << \"\\n\";\n",
        "\t\t}\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing preprocess_data.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pWS3hlAecOc"
      },
      "source": [
        "## Classifier Math"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rK_tmB-xbZKp",
        "outputId": "ec9ce9b0-249a-452c-ef0f-feda5322ac81"
      },
      "source": [
        "%%writefile classifier_math.cuh\n",
        "#ifndef classifier_math_H\n",
        "#define classifier_math_H\n",
        "#include \"cublas_v2.h\"\n",
        "#include \"fmatrix.cuh\"\n",
        "\n",
        "/** Returns a random float between min and max (including). */\n",
        "float float_rand( float min, float max );\n",
        "\n",
        "/** Initialize W with Xavier's method,\n",
        " *  scaled by a. */\n",
        "void xavier_weight_init(float a, fmatrix W);\n",
        "\n",
        "/** Compute the softmax for each column of Z and store in P **/\n",
        "void stable_softmax_col(fmatrix P,fmatrix Z); \n",
        "\n",
        "/** Data normalization **/\n",
        "void normalization(fmatrix X, fmatrix X_test);\n",
        "\n",
        "///////////////////////////////////\n",
        "// TO BE COMPLETED\n",
        "// ... add your matrix math here\n",
        "///////////////////////////////////\n",
        "\n",
        "void mat_transp_mul(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C, int inv);\n",
        "/** Z = a*X^T*Y */\n",
        "void fmatrix_transp_mul(fmatrix Z,float a,fmatrix X,fmatrix Y);\n",
        "\n",
        "\n",
        "#endif"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing classifier_math.cuh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgXwgv6Bbo-I",
        "outputId": "8228dbac-e08b-4ff4-94f3-45985cdef19b"
      },
      "source": [
        "%%writefile classifier_math.cu\n",
        "#include \"classifier_math.cuh\"\n",
        "#include \"cuda_stuff.cuh\"\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "#include <math.h>\n",
        "#include <assert.h>\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "\n",
        "__global__ \n",
        "void fmatrix_transp_mul_kernel(fmatrix Z,float a,fmatrix X,fmatrix Y) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    int j = idx / Z.rows;\n",
        "    int i = idx % Z.rows;\n",
        "    // printf(\"(%d,%d) \\n\",i,j);\n",
        "    if (i < Z.rows && j < Z.cols ){\n",
        "        getfm(Z,i,j) = 0.0;\n",
        "        for (int k = 0; k< X.rows ; ++k) {\n",
        "          // printf(\"%f + %f * %f * %f\\n\", getfm(Z,i,j), a,getfm(X,k,i),getfm(Y,k,j));\n",
        "          getfm(Z,i,j) += a*getfm(X,k,i)*getfm(Y,k,j);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void fmatrix_transp_mul(fmatrix Z,float a,fmatrix X,fmatrix Y) {\n",
        "    assert(Z.rows == X.cols);\n",
        "    assert(Z.cols == Y.cols);\n",
        "    assert(X.rows == Y.rows);\n",
        "\n",
        "    // printf(\"Z(%d,%d) elements %d \\n\",Z.rows,Z.cols,fmatrix_elements(Z));\n",
        "\n",
        "    int threadsPerBlock = fmatrix_elements(Z);\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    fmatrix_transp_mul_kernel<<< blocksPerGrid, threadsPerBlock >>>(Z,a,X,Y);\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "}\n",
        "\n",
        "void mat_transp_mul(float alpha, fmatrix d_A, fmatrix d_B, float beta, fmatrix d_C, int inv){\n",
        "  /* TODO */\n",
        "  int m=d_A.rows;\n",
        "  int k=d_A.cols;\n",
        "  int n=d_B.cols;\n",
        "  if (inv == 0) {\n",
        "    m=d_A.cols;\n",
        "    k=d_A.rows;\n",
        "    n=d_B.cols;\n",
        "    }\n",
        "  else if (inv ==1) {\n",
        "    n=d_B.rows;\n",
        "  }\n",
        "  \n",
        "  int lda=d_A.rows,ldb=d_B.rows,ldc=d_C.rows;\n",
        "  \n",
        "  const float *alf = &alpha;\n",
        "  const float *bet= &beta;\n",
        " \n",
        "  // Create a handle for CUBLAS\n",
        "  cublasHandle_t handle;\n",
        "  cublasCreate(&handle);\n",
        "\n",
        "\n",
        "  //clock_t start, end;\n",
        "  //float cpu_time_used;\n",
        "  //start = clock();\n",
        "  // Do the actual multiplication\n",
        "  if (inv == 0)\n",
        "  {cublasSgemm(handle, CUBLAS_OP_T, CUBLAS_OP_N, m, n, k, alf, d_A.data, lda, d_B.data, ldb, bet, d_C.data, ldc);}\n",
        "  else\n",
        "  {   \n",
        "      cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_T, m, n, k, alf, d_A.data, lda, d_B.data, ldb, bet, d_C.data, ldc);}\n",
        " \n",
        "  // Destroy the handle\n",
        "  cublasDestroy(handle);\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Auxiliary function\n",
        "/////////////////////////////////////////////////////////\n",
        "// generate random numbers in interval [min,max]\n",
        "float float_rand( float min, float max )\n",
        "{\n",
        "    float scale = rand() / (float) RAND_MAX; /* [0, 1.0] */\n",
        "    return min + scale * ( max - min );      /* [min, max] */\n",
        "}\n",
        "\n",
        "void xavier_weight_init(float a, fmatrix W){\n",
        "    for (int j = 0; j < W.rows  ; ++j) {\n",
        "      for (int i = 0; i < W.cols  ; ++i) {\n",
        "          getfm(W,j,i) = a * (1.0/sqrt(W.cols+W.rows)) * float_rand(-1.0,1.0);\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__\n",
        "void stable_softmax_col_kernel(float *Z, float *P, int nb_ColZ, int nb_LigneZ) {\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\n",
        "    float s=0;\n",
        "  \n",
        "    \n",
        "    if (col < nb_ColZ){\n",
        "        \n",
        "        float Z_max=Z[IDX2C(0,col,nb_LigneZ)];\n",
        "        for (int k=1; k<nb_LigneZ; k++){\n",
        "            if (Z[IDX2C(k,col,nb_LigneZ)] > Z_max)\n",
        "            {\n",
        "                Z_max=Z[IDX2C(k,col,nb_LigneZ)];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for (int k=0; k<nb_LigneZ; k++){\n",
        "            s+=exp(Z[col*nb_LigneZ+k]-Z_max);\n",
        "        }\n",
        "        for (int k=0; k<nb_LigneZ; k++) {\n",
        "            P[col*nb_LigneZ + k]=exp(Z[col*nb_LigneZ+k]-Z_max)/s;\n",
        "        }\n",
        "    }\n",
        "  \n",
        "}\n",
        "\n",
        "void stable_softmax_col(fmatrix P,fmatrix Z) {\n",
        "    assert(P.cols==Z.cols);\n",
        "    assert(P.rows==Z.rows);\n",
        "    \n",
        "    int threadsPerBlock = Z.cols;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    stable_softmax_col_kernel <<< blocksPerGrid, threadsPerBlock >>>(Z.data, P.data, Z.cols, Z.rows);\n",
        "\n",
        "    \n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    device_synchronize();\n",
        "}\n",
        "\n",
        "\n",
        "//Normalization\n",
        "\n",
        "\n",
        "__global__\n",
        "void normalization_kernel(float *X, float *X_test, int nb_Ligne, int nb_col, int nb_col_test) {\n",
        "    int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    \n",
        "    float mu=0;\n",
        "    float sigma=0;\n",
        "    \n",
        "    if (row < nb_Ligne && row >0){\n",
        "        \n",
        "        for (int k=0; k<nb_col; k++){\n",
        "            mu+=X[IDX2C(row,k,nb_Ligne)];\n",
        "        }\n",
        "        mu/=nb_col;\n",
        "        \n",
        "      \n",
        "        for (int k=0; k<nb_col; k++) {\n",
        "            sigma+=(X[IDX2C(row,k,nb_Ligne)]-mu)*(X[IDX2C(row,k,nb_Ligne)]-mu);\n",
        "        }\n",
        "        sigma=sqrt(sigma/nb_col);\n",
        "       \n",
        "        for (int k=0; k<nb_col; k++){\n",
        "            X[IDX2C(row,k,nb_Ligne)]=(1/sigma)*(X[IDX2C(row,k,nb_Ligne)]-mu);\n",
        "        }\n",
        "\n",
        "        for (int k=0; k<nb_col_test; k++){\n",
        "            X_test[IDX2C(row,k,nb_Ligne)]=(1/sigma)*(X_test[IDX2C(row,k,nb_Ligne)]-mu);\n",
        "        }\n",
        "    }\n",
        "  \n",
        "}\n",
        "\n",
        "void normalization(fmatrix X, fmatrix X_test) {\n",
        "    \n",
        "    \n",
        "    int threadsPerBlock = X.rows;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "  \n",
        "    normalization_kernel <<< blocksPerGrid, threadsPerBlock >>>(X.data, X_test.data, X.rows, X.cols, X_test.cols);\n",
        "\n",
        "    \n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "    device_synchronize();\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing classifier_math.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6G22rDKR3rv"
      },
      "source": [
        "## Evaluating Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd7Vmzo72hpC",
        "outputId": "40d65069-7fd4-4949-db1e-ead845dc2a51"
      },
      "source": [
        "%%writefile evaluate_accuracy.cuh\n",
        "\n",
        "/** Evaluate the accuracy of a linear classifier with D x M weight\n",
        " *  matrix W, using D x N input data X and M x N output labels Y.\n",
        " *  Z is a temporary matrix with dimensions M x N,\n",
        " *  which must be previously allocated.\n",
        " */\n",
        "float evaluate_accuracy(fmatrix d_W,fmatrix d_X,fmatrix d_Y,fmatrix d_Z);\n",
        "\n",
        "/** Compute the logloss given M x N matrices of \n",
        " *  probabilities P and output labels Y\n",
        " *  and stores it in J.\n",
        " *  J is a matrix with dimensions 1 x 1,\n",
        " *  which must be previously allocated.\n",
        " *  logloss = sum_j -Y(k,j)*log(P(k,j))\n",
        " */\n",
        "float evaluate_logloss(fmatrix d_P,fmatrix d_Y);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing evaluate_accuracy.cuh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0Z-9B4a2dwg",
        "outputId": "b6e6eaf0-a493-4960-9c60-fa71f6d06198"
      },
      "source": [
        "%%writefile evaluate_accuracy.cu\n",
        "#include \"fmatrix.cuh\"\n",
        "#include \"classifier_math.cuh\"\n",
        "#include <assert.h>\n",
        "\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__ \n",
        "void evaluate_accuracy_kernel(fmatrix d_Y,fmatrix d_Z,int* count) {\n",
        "    int idx = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "    if (idx < d_Z.cols){\n",
        "        float z_max = getfm(d_Z,0,idx);\n",
        "        int i_max = 0;\n",
        "        for (int i = 1; i < d_Z.rows; ++i) {\n",
        "          if (getfm(d_Z,i,idx)>z_max) {\n",
        "                z_max = getfm(d_Z,i,idx);\n",
        "                i_max = i;\n",
        "          }\n",
        "        }\n",
        "      if (getfm(d_Y,i_max,idx)>=0.5f) {\n",
        "          atomicAdd(count,1);\n",
        "      }\n",
        "    }    \n",
        "}\n",
        "\n",
        "float evaluate_accuracy(fmatrix d_W,fmatrix d_X,fmatrix d_Y,fmatrix d_Z) {\n",
        "    assert(d_Y.cols == d_Z.cols);\n",
        "    assert(d_Y.rows == d_Z.rows);\n",
        "\n",
        "  // Z = W^T X\n",
        "  /* Multiplication on the device*/\n",
        "  //fmatrix_transp_mul(d_Z,1.0,d_W,d_X);\n",
        "  \n",
        "  mat_transp_mul(1.0, d_W, d_X, 0.0, d_Z,0);\n",
        "  device_synchronize();\n",
        "  ////////////////////////////////\n",
        "  // compute Z = W^T X'\n",
        "  // --> each column of Z corresponds to one input\n",
        "  ////////////////////////////////\n",
        "  // For each column z of Z, \n",
        "  // find argmax_k z_k\n",
        "  ////////////////////////////////\n",
        "  int true_class = 0;\n",
        "\n",
        "  int* d_count = 0;\n",
        "  gpuErrchk(cudaMalloc((void **)&d_count, sizeof(int)));\n",
        "  gpuErrchk( \n",
        "        cudaMemcpy( d_count, &true_class, sizeof(int), cudaMemcpyHostToDevice )\n",
        "  );\n",
        "\n",
        "    int threadsPerBlock = d_Z.cols;\n",
        "    int blocksPerGrid = 1;\n",
        "    if (threadsPerBlock > THREADS_PER_BLOCK){\n",
        "        blocksPerGrid = (threadsPerBlock-1)/THREADS_PER_BLOCK+1;\n",
        "        threadsPerBlock = THREADS_PER_BLOCK;\n",
        "    }\n",
        "    evaluate_accuracy_kernel<<< blocksPerGrid, threadsPerBlock >>>(d_Y,d_Z,d_count);\n",
        "    device_synchronize();\n",
        "    gpuErrchk( cudaPeekAtLastError() );\n",
        "\n",
        "  gpuErrchk(\n",
        "          cudaMemcpy(&true_class, d_count, sizeof(int), cudaMemcpyDeviceToHost )\n",
        "  );\n",
        "\n",
        "  //printf(\"Correct results: %d out of %d\\n\",true_class,nb_tested);\n",
        "  //printf(\"Accuracy: %f\\n\",(float)true_class/(float)nb_tested);\n",
        "  return (float)true_class/(float)d_Z.cols;\n",
        "}\n",
        "\n",
        "\n",
        "float evaluate_logloss(fmatrix h_P,fmatrix h_Y) {\n",
        "    assert(h_Y.cols == h_P.cols);\n",
        "    assert(h_Y.rows == h_P.rows);\n",
        "    \n",
        "    float J = 0.0;\n",
        "\n",
        "    for (int i =0 ; i <h_P.cols; i++)\n",
        "    {\n",
        "    for (int k = 0 ; k < h_P.rows; k++)\n",
        "        {\n",
        "            J+=getfm(h_Y,k,i)* log(getfm(h_P,k,i));\n",
        "           \n",
        "        }\n",
        "    }\n",
        "  J=-J/h_P.cols;\n",
        "  return J;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing evaluate_accuracy.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AwZh8WULi_F"
      },
      "source": [
        "## Linear Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWO5p1NeHE9n",
        "outputId": "057cf754-e05b-45a1-f99f-67bebf3de8c4"
      },
      "source": [
        "%%writefile linear_classification.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <iomanip>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "#include <fstream>\n",
        "\n",
        "/*Matrix multiplication functions and other auxiliary functions*/\n",
        "#include \"fmatrix.cuh\"\n",
        "//#include \"sgemm.cuh\"\n",
        "#include \"read_csv.cuh\"\n",
        "#include \"preprocess_data.cuh\"\n",
        "#include \"classifier_math.cuh\"\n",
        "#include \"evaluate_accuracy.cuh\"\n",
        "/* Includes, cuda */\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "//Number of thread per block\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "/* Constants for housing data set */\n",
        "#define data_columns  (9)\n",
        "#define above_threshold (265000.0)\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Number of rows in arrays to print for debugging\n",
        "/////////////////////////////////////////////////////////\n",
        "#define print_rows (10)\n",
        "\n",
        "\n",
        "/////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "/////////////////////////////////////////////////////////\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    size_t N_train =12000; //12000; // points for training (Google: 12000)\n",
        "    size_t N_test =5000; // 5000; // points for validation (Google: 5000)\n",
        "    size_t N = N_train;\n",
        "    size_t Nall = N_train+N_test;\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Reading the data set\n",
        "    /////////////////////////////////////////////////////////\n",
        "    fmatrix alldata = fmatrix_create_on_host(Nall,data_columns);\n",
        "    read_csv(\"sample_data/california_housing_train.csv\",alldata.data,Nall,data_columns);\n",
        "    //fmatrix_print(alldata);\n",
        "\n",
        "    size_t D = data_columns-1+1; // remove output column, add column with const. 1.0\n",
        "    size_t M = 2; // number of labels (one-hot encoding)\n",
        "    fmatrix Xall = fmatrix_create_on_host(D,Nall);\n",
        "    fmatrix Yall = fmatrix_create_on_host(M,Nall);\n",
        "    get_inputs_and_labels(alldata.data,&Xall.data,&Yall.data,Nall,data_columns,D,M);\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Inputs and labels are now available in X and Y.\n",
        "    // Each input is a column in X; X is of dimension D x N\n",
        "    // each label is a column in Y; Y is of dimension M x N\n",
        "    /////////////////////////////////////////////////////////\n",
        "     \n",
        "    // Logfile\n",
        "    FILE* fp = fopen(\"log.txt\", \"w\");\n",
        "    \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Parameters for Stochastic Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    int nb_iter = 500; //10;\n",
        "    int periods = nb_iter; // reporting period\n",
        "    int batch_size = N; // N;\n",
        "    float learning_rate = 1e-1;\n",
        " \n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Memory Allocation and Initialization\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // take X,Y to be the first N columns of all data\n",
        "    fmatrix h_X = fmatrix_subcolumns(Xall,0,N);\n",
        "    fmatrix h_Y = fmatrix_subcolumns(Yall,0,N);\n",
        "    fmatrix h_Xtest = fmatrix_subcolumns(Xall,N,Nall);\n",
        "    fmatrix h_Ytest = fmatrix_subcolumns(Yall,N,Nall);\n",
        "    fmatrix h_W = fmatrix_create_on_host(D,M);\n",
        "    fmatrix h_J = fmatrix_create_on_host(1,1);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Initializing Weight Matrix \n",
        "    // its dimension is D x M\n",
        "    /////////////////////////////////////////////////////////\n",
        "    xavier_weight_init(1,h_W);\n",
        " \n",
        "    //////////////////////////////\n",
        "    // Copy data to device      //\n",
        "    //////////////////////////////\n",
        "    fmatrix d_X = fmatrix_copy_to_device(h_X);\n",
        "    //printf(\"X_=\");fmatrix_device_print(d_X);\n",
        "    \n",
        "    //printf(\"X_test_norm=\");fmatrix_device_print(d_X);\n",
        "    fmatrix d_Y = fmatrix_copy_to_device(h_Y);\n",
        "    fmatrix d_Xtest = fmatrix_copy_to_device(h_Xtest);\n",
        "    \n",
        "    normalization(d_X,d_Xtest);\n",
        "    \n",
        "    fmatrix d_Ytest = fmatrix_copy_to_device(h_Ytest);\n",
        "    fmatrix d_W = fmatrix_copy_to_device(h_W);\n",
        "    fmatrix d_J = fmatrix_copy_to_device(h_J);\n",
        " \n",
        "    /////////////////////////////////////////\n",
        "    // Create auxiliary matrices on device //\n",
        "    /////////////////////////////////////////\n",
        "    fmatrix d_Z = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_P = fmatrix_create_on_device(M,batch_size);\n",
        "    fmatrix d_G = fmatrix_create_on_device(D,M);\n",
        "    // auxiliary matrix for computing Z=W^T X on test data\n",
        "    fmatrix d_Ztest = fmatrix_create_on_device(M,d_Xtest.cols);\n",
        "\n",
        "    /////////////////////////////////////////////////////////\n",
        "    // Batch Gradient Descent\n",
        "    /////////////////////////////////////////////////////////\n",
        "    //fmatrix_device_print(d_X);\n",
        "    //fmatrix_device_print(d_W);\n",
        " \n",
        "     /* Evaluate the accuracy */\n",
        "    \n",
        "    float accuracy = 0;\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"initial accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    \n",
        "\n",
        "    float J = 0;\n",
        " \n",
        "    clock_t tstart_total, tend;\n",
        "    tstart_total = clock();\n",
        "\n",
        "    //int batch_pointer = 0;\n",
        "    for (int m = 0; m < nb_iter; m++ ) {\n",
        "        //printf(\"W:\\n\");fmatrix_device_print(d_W);\n",
        "        //printf(\"X:\\n\");fmatrix_device_print(d_X);\n",
        "      \n",
        "      ////////////////////////////////\n",
        "      // compute Z = W^T X\n",
        "      // --> each column z of Z corresponds to one column x of X\n",
        "\n",
        "      ////////////////////////////////\n",
        "\n",
        "      \n",
        "      ///////////////////////////////////\n",
        "        \n",
        "        mat_transp_mul(1.0, d_W, d_X, 0.0, d_Z, 0);\n",
        "        //printf(\"Z:\\n\"); fmatrix_device_print(d_Z);\n",
        "        //fmatrix d_Z2 = fmatrix_create_on_device(M,batch_size);\n",
        "        //fmatrix_transp_mul(d_Z2,1.0,d_W,d_X);\n",
        "       //printf(\"Z2:\\n\"); fmatrix_device_print(d_Z2);\n",
        "\n",
        "      \n",
        "      ///////////////////////////////////\n",
        "      \n",
        "      \n",
        "\n",
        "      ////////////////////////////////\n",
        "      // For each column z of Z, compute activation p(z);\n",
        "      // then update W\n",
        "      ////////////////////////////////\n",
        "\n",
        "      // compute softmax per column of Z and store in Z\n",
        "\n",
        "    ///////////////////////////////////\n",
        "        stable_softmax_col(d_P, d_Z);\n",
        "        //printf(\"Z:\\n\"); fmatrix_device_print(d_Z);\n",
        "        //printf(\"P:\\n\"); fmatrix_device_print(d_P);\n",
        "        fmatrix h_P = fmatrix_copy_to_host(d_P);\n",
        "        \n",
        "    ///////////////////////////////////\n",
        "    \n",
        "      // evaluate logloss (for reporting only)\n",
        "\n",
        "    ///////////////////////////////////\n",
        "    \n",
        "        J=evaluate_logloss(h_P, h_Y);\n",
        "        //printf (\"Jiiiiiii %f \\n\", J);\n",
        "    ///////////////////////////////////\n",
        "        \n",
        "      \n",
        "      // Q:=P-Y\n",
        "      // compute gradient G = 1/batch_size XQ^T\n",
        "      // ... possibly work with G here ...\n",
        "      // update weights W = W - learning_rate*G\n",
        "\n",
        "    ///////////////////////////////////\n",
        "        fmatrix h_Q = fmatrix_create_on_host(M,batch_size);\n",
        "        for (int i = 0 ; i < h_Q.rows; i++){\n",
        "            for (int j = 0 ; j<h_Q.cols; j++){\n",
        "                h_Q.data[IDX2C(i,j,h_Q.rows)]= getfm(h_P,i,j)-getfm(h_Y,i,j);\n",
        "      }\n",
        "    }\n",
        "\n",
        "        \n",
        "        //fmatrix_print(h_Q);\n",
        "        fmatrix d_Q = fmatrix_copy_to_device(h_Q);\n",
        "       \n",
        "\n",
        "        mat_transp_mul(1.0, d_X, d_Q, 0.0, d_G, 1);\n",
        "\n",
        "        fmatrix h_G = fmatrix_copy_to_host(d_G);\n",
        "        //printf(\"G:\\n\"); fmatrix_print(h_G);\n",
        "        for (int i = 0 ; i < h_G.rows; i++){\n",
        "          for (int j = 0 ; j<h_G.cols; j++){\n",
        "              h_G.data[IDX2C(i,j,h_G.rows)]= getfm(h_G,i,j)/batch_size;\n",
        "          }\n",
        "        }\n",
        "        fmatrix_data_to_device(h_G,d_G);\n",
        "        for (int i = 0 ; i < h_W.rows; i++){\n",
        "          for (int j = 0 ; j<h_W.cols; j++){\n",
        "              h_W.data[IDX2C(i,j,h_W.rows)]=getfm(h_W,i,j)- learning_rate* getfm(h_G,i,j);\n",
        "          }\n",
        "        }\n",
        "    \n",
        "        \n",
        "   \n",
        "      fmatrix_data_to_device(h_W,d_W);\n",
        "      //printf(\"W_h:\\n\");fmatrix_print(h_W);\n",
        "      \n",
        "      \n",
        "      \n",
        "      // For reporting, compute logloss and accuracy\n",
        "      \n",
        "      if (m%(nb_iter/periods)==0) {\n",
        "       \n",
        "        float accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "        printf(\"iter: %d, logloss: %f, accuracy: %f\\n\",m,J, accuracy);\n",
        "        fprintf(fp, \"%f,%f\\n\", J, accuracy);\n",
        "      }\n",
        "\n",
        "    }\n",
        "    tend = clock();\n",
        "    float duration = ((float)(tend-tstart_total))/CLOCKS_PER_SEC;\n",
        "    printf(\"Duration (s): %f\\n\",duration);\n",
        "    /* Evaluate the accuracy */\n",
        "    accuracy = evaluate_accuracy(d_W,d_Xtest,d_Ytest,d_Ztest);\n",
        "    printf(\"final accuracy: %f\\n\",accuracy);\n",
        "\n",
        "    printf(\"final weights: \\n\");\n",
        "    fmatrix_device_print(d_W);\n",
        "\n",
        "    /* Memory clean up */\n",
        "    /** No need to free h_X, h_Y, h_Xtest, h_Ytest since \n",
        "     *  they all point to Xall \n",
        "     */\n",
        "    fmatrix_free_on_host(&h_W);\n",
        "    fmatrix_free_on_host(&Xall);\n",
        "    fmatrix_free_on_host(&Yall);\n",
        "\n",
        "    fmatrix_free_on_device(&d_X);\n",
        "    fmatrix_free_on_device(&d_Y);\n",
        "    fmatrix_free_on_device(&d_Xtest);\n",
        "    fmatrix_free_on_device(&d_Ytest);\n",
        "    fmatrix_free_on_device(&d_W);\n",
        "    fmatrix_free_on_device(&d_Z);\n",
        "    fmatrix_free_on_device(&d_J);\n",
        " \n",
        "    // Close log file\n",
        "    fclose(fp);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing linear_classification.cu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrATC8s9LsDw"
      },
      "source": [
        "# Compiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z52xd0NMRKXb"
      },
      "source": [
        "!nvcc -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver linear_classification.cu read_csv.cu preprocess_data.cu evaluate_accuracy.cu fmatrix.cu classifier_math.cu cuda_stuff.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZVqTfXcLvPr"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV_vFkIT7fV4",
        "outputId": "62a0bbbe-1cc5-4a52-bc7c-c767af51f532"
      },
      "source": [
        "#@title\n",
        "%%time\n",
        "!./a.out "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 17000 rows.\n",
            "Allocated memory for inputs: 17000 rows, 9 columns.\n",
            "Allocated memory for labels: 17000 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t-114.58\t-114.59\t-114.59\t-114.6\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t33.61\t34.83\t33.61\t34.83\t\n",
            "15\t19\t17\t14\t20\t29\t25\t41\t34\t46\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t2907\t812\t4789\t1497\t\n",
            "1283\t1901\t174\t337\t326\t236\t680\t168\t1175\t309\t\n",
            "1015\t1129\t333\t515\t624\t671\t1841\t375\t3134\t787\t\n",
            "472\t463\t117\t226\t262\t239\t633\t158\t1056\t271\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t2.6768\t1.7083\t2.1782\t2.1908\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "initial accuracy: 0.340200\n",
            "iter: 0, logloss: 0.880316, accuracy: 0.340000\n",
            "iter: 1, logloss: 0.830006, accuracy: 0.340200\n",
            "iter: 2, logloss: 0.785198, accuracy: 0.342600\n",
            "iter: 3, logloss: 0.745490, accuracy: 0.370600\n",
            "iter: 4, logloss: 0.710397, accuracy: 0.434200\n",
            "iter: 5, logloss: 0.679411, accuracy: 0.529800\n",
            "iter: 6, logloss: 0.652040, accuracy: 0.622000\n",
            "iter: 7, logloss: 0.627811, accuracy: 0.691800\n",
            "iter: 8, logloss: 0.606301, accuracy: 0.730200\n",
            "iter: 9, logloss: 0.587134, accuracy: 0.743200\n",
            "iter: 10, logloss: 0.569993, accuracy: 0.743600\n",
            "iter: 11, logloss: 0.554603, accuracy: 0.743600\n",
            "iter: 12, logloss: 0.540726, accuracy: 0.739800\n",
            "iter: 13, logloss: 0.528171, accuracy: 0.735400\n",
            "iter: 14, logloss: 0.516767, accuracy: 0.733000\n",
            "iter: 15, logloss: 0.506375, accuracy: 0.730200\n",
            "iter: 16, logloss: 0.496874, accuracy: 0.728600\n",
            "iter: 17, logloss: 0.488162, accuracy: 0.727200\n",
            "iter: 18, logloss: 0.480151, accuracy: 0.725800\n",
            "iter: 19, logloss: 0.472766, accuracy: 0.724800\n",
            "iter: 20, logloss: 0.465936, accuracy: 0.723800\n",
            "iter: 21, logloss: 0.459610, accuracy: 0.723600\n",
            "iter: 22, logloss: 0.453735, accuracy: 0.723800\n",
            "iter: 23, logloss: 0.448267, accuracy: 0.723600\n",
            "iter: 24, logloss: 0.443169, accuracy: 0.723600\n",
            "iter: 25, logloss: 0.438407, accuracy: 0.723800\n",
            "iter: 26, logloss: 0.433951, accuracy: 0.723600\n",
            "iter: 27, logloss: 0.429771, accuracy: 0.723800\n",
            "iter: 28, logloss: 0.425848, accuracy: 0.724000\n",
            "iter: 29, logloss: 0.422157, accuracy: 0.724200\n",
            "iter: 30, logloss: 0.418679, accuracy: 0.724400\n",
            "iter: 31, logloss: 0.415401, accuracy: 0.724600\n",
            "iter: 32, logloss: 0.412301, accuracy: 0.724800\n",
            "iter: 33, logloss: 0.409372, accuracy: 0.724800\n",
            "iter: 34, logloss: 0.406597, accuracy: 0.725000\n",
            "iter: 35, logloss: 0.403964, accuracy: 0.725400\n",
            "iter: 36, logloss: 0.401464, accuracy: 0.726400\n",
            "iter: 37, logloss: 0.399088, accuracy: 0.727000\n",
            "iter: 38, logloss: 0.396828, accuracy: 0.727400\n",
            "iter: 39, logloss: 0.394675, accuracy: 0.727800\n",
            "iter: 40, logloss: 0.392620, accuracy: 0.727800\n",
            "iter: 41, logloss: 0.390660, accuracy: 0.728800\n",
            "iter: 42, logloss: 0.388785, accuracy: 0.729400\n",
            "iter: 43, logloss: 0.386994, accuracy: 0.730000\n",
            "iter: 44, logloss: 0.385277, accuracy: 0.730000\n",
            "iter: 45, logloss: 0.383634, accuracy: 0.730400\n",
            "iter: 46, logloss: 0.382058, accuracy: 0.730600\n",
            "iter: 47, logloss: 0.380545, accuracy: 0.730600\n",
            "iter: 48, logloss: 0.379091, accuracy: 0.731600\n",
            "iter: 49, logloss: 0.377695, accuracy: 0.732200\n",
            "iter: 50, logloss: 0.376349, accuracy: 0.733000\n",
            "iter: 51, logloss: 0.375054, accuracy: 0.733200\n",
            "iter: 52, logloss: 0.373807, accuracy: 0.733600\n",
            "iter: 53, logloss: 0.372606, accuracy: 0.733800\n",
            "iter: 54, logloss: 0.371445, accuracy: 0.734200\n",
            "iter: 55, logloss: 0.370325, accuracy: 0.734600\n",
            "iter: 56, logloss: 0.369244, accuracy: 0.735400\n",
            "iter: 57, logloss: 0.368197, accuracy: 0.736200\n",
            "iter: 58, logloss: 0.367185, accuracy: 0.736800\n",
            "iter: 59, logloss: 0.366205, accuracy: 0.737400\n",
            "iter: 60, logloss: 0.365257, accuracy: 0.737800\n",
            "iter: 61, logloss: 0.364338, accuracy: 0.738400\n",
            "iter: 62, logloss: 0.363446, accuracy: 0.738800\n",
            "iter: 63, logloss: 0.362582, accuracy: 0.738800\n",
            "iter: 64, logloss: 0.361742, accuracy: 0.739200\n",
            "iter: 65, logloss: 0.360929, accuracy: 0.739200\n",
            "iter: 66, logloss: 0.360137, accuracy: 0.740400\n",
            "iter: 67, logloss: 0.359368, accuracy: 0.740800\n",
            "iter: 68, logloss: 0.358621, accuracy: 0.741400\n",
            "iter: 69, logloss: 0.357892, accuracy: 0.742000\n",
            "iter: 70, logloss: 0.357182, accuracy: 0.742400\n",
            "iter: 71, logloss: 0.356493, accuracy: 0.742800\n",
            "iter: 72, logloss: 0.355821, accuracy: 0.743800\n",
            "iter: 73, logloss: 0.355166, accuracy: 0.744200\n",
            "iter: 74, logloss: 0.354528, accuracy: 0.744600\n",
            "iter: 75, logloss: 0.353904, accuracy: 0.744800\n",
            "iter: 76, logloss: 0.353296, accuracy: 0.745400\n",
            "iter: 77, logloss: 0.352704, accuracy: 0.745600\n",
            "iter: 78, logloss: 0.352125, accuracy: 0.746000\n",
            "iter: 79, logloss: 0.351558, accuracy: 0.746200\n",
            "iter: 80, logloss: 0.351005, accuracy: 0.746400\n",
            "iter: 81, logloss: 0.350463, accuracy: 0.746800\n",
            "iter: 82, logloss: 0.349935, accuracy: 0.747200\n",
            "iter: 83, logloss: 0.349419, accuracy: 0.747200\n",
            "iter: 84, logloss: 0.348912, accuracy: 0.747400\n",
            "iter: 85, logloss: 0.348416, accuracy: 0.747400\n",
            "iter: 86, logloss: 0.347931, accuracy: 0.747600\n",
            "iter: 87, logloss: 0.347456, accuracy: 0.747600\n",
            "iter: 88, logloss: 0.346990, accuracy: 0.748200\n",
            "iter: 89, logloss: 0.346533, accuracy: 0.748800\n",
            "iter: 90, logloss: 0.346086, accuracy: 0.749200\n",
            "iter: 91, logloss: 0.345648, accuracy: 0.749200\n",
            "iter: 92, logloss: 0.345217, accuracy: 0.749800\n",
            "iter: 93, logloss: 0.344795, accuracy: 0.750400\n",
            "iter: 94, logloss: 0.344381, accuracy: 0.750600\n",
            "iter: 95, logloss: 0.343974, accuracy: 0.751000\n",
            "iter: 96, logloss: 0.343573, accuracy: 0.751200\n",
            "iter: 97, logloss: 0.343181, accuracy: 0.751400\n",
            "iter: 98, logloss: 0.342796, accuracy: 0.751400\n",
            "iter: 99, logloss: 0.342417, accuracy: 0.751600\n",
            "iter: 100, logloss: 0.342044, accuracy: 0.752000\n",
            "iter: 101, logloss: 0.341678, accuracy: 0.752800\n",
            "iter: 102, logloss: 0.341319, accuracy: 0.752800\n",
            "iter: 103, logloss: 0.340965, accuracy: 0.753000\n",
            "iter: 104, logloss: 0.340616, accuracy: 0.753600\n",
            "iter: 105, logloss: 0.340273, accuracy: 0.753800\n",
            "iter: 106, logloss: 0.339935, accuracy: 0.754200\n",
            "iter: 107, logloss: 0.339603, accuracy: 0.755000\n",
            "iter: 108, logloss: 0.339276, accuracy: 0.755000\n",
            "iter: 109, logloss: 0.338955, accuracy: 0.755400\n",
            "iter: 110, logloss: 0.338637, accuracy: 0.755800\n",
            "iter: 111, logloss: 0.338325, accuracy: 0.756000\n",
            "iter: 112, logloss: 0.338018, accuracy: 0.756200\n",
            "iter: 113, logloss: 0.337713, accuracy: 0.756800\n",
            "iter: 114, logloss: 0.337415, accuracy: 0.757600\n",
            "iter: 115, logloss: 0.337120, accuracy: 0.757600\n",
            "iter: 116, logloss: 0.336828, accuracy: 0.757800\n",
            "iter: 117, logloss: 0.336541, accuracy: 0.758600\n",
            "iter: 118, logloss: 0.336259, accuracy: 0.758600\n",
            "iter: 119, logloss: 0.335979, accuracy: 0.759200\n",
            "iter: 120, logloss: 0.335703, accuracy: 0.759400\n",
            "iter: 121, logloss: 0.335433, accuracy: 0.759600\n",
            "iter: 122, logloss: 0.335164, accuracy: 0.760200\n",
            "iter: 123, logloss: 0.334898, accuracy: 0.760600\n",
            "iter: 124, logloss: 0.334637, accuracy: 0.760600\n",
            "iter: 125, logloss: 0.334379, accuracy: 0.760600\n",
            "iter: 126, logloss: 0.334123, accuracy: 0.761000\n",
            "iter: 127, logloss: 0.333872, accuracy: 0.761600\n",
            "iter: 128, logloss: 0.333622, accuracy: 0.761800\n",
            "iter: 129, logloss: 0.333377, accuracy: 0.762200\n",
            "iter: 130, logloss: 0.333133, accuracy: 0.762400\n",
            "iter: 131, logloss: 0.332893, accuracy: 0.762800\n",
            "iter: 132, logloss: 0.332656, accuracy: 0.763400\n",
            "iter: 133, logloss: 0.332421, accuracy: 0.764200\n",
            "iter: 134, logloss: 0.332188, accuracy: 0.764400\n",
            "iter: 135, logloss: 0.331958, accuracy: 0.764600\n",
            "iter: 136, logloss: 0.331732, accuracy: 0.764800\n",
            "iter: 137, logloss: 0.331507, accuracy: 0.765000\n",
            "iter: 138, logloss: 0.331284, accuracy: 0.765200\n",
            "iter: 139, logloss: 0.331065, accuracy: 0.765400\n",
            "iter: 140, logloss: 0.330847, accuracy: 0.765400\n",
            "iter: 141, logloss: 0.330632, accuracy: 0.765800\n",
            "iter: 142, logloss: 0.330418, accuracy: 0.766200\n",
            "iter: 143, logloss: 0.330208, accuracy: 0.766600\n",
            "iter: 144, logloss: 0.330000, accuracy: 0.767400\n",
            "iter: 145, logloss: 0.329793, accuracy: 0.767600\n",
            "iter: 146, logloss: 0.329589, accuracy: 0.767800\n",
            "iter: 147, logloss: 0.329386, accuracy: 0.768200\n",
            "iter: 148, logloss: 0.329185, accuracy: 0.768400\n",
            "iter: 149, logloss: 0.328987, accuracy: 0.768800\n",
            "iter: 150, logloss: 0.328789, accuracy: 0.769000\n",
            "iter: 151, logloss: 0.328595, accuracy: 0.769200\n",
            "iter: 152, logloss: 0.328400, accuracy: 0.769200\n",
            "iter: 153, logloss: 0.328210, accuracy: 0.769600\n",
            "iter: 154, logloss: 0.328020, accuracy: 0.769600\n",
            "iter: 155, logloss: 0.327833, accuracy: 0.770000\n",
            "iter: 156, logloss: 0.327646, accuracy: 0.770200\n",
            "iter: 157, logloss: 0.327462, accuracy: 0.770600\n",
            "iter: 158, logloss: 0.327280, accuracy: 0.770800\n",
            "iter: 159, logloss: 0.327099, accuracy: 0.770800\n",
            "iter: 160, logloss: 0.326919, accuracy: 0.771000\n",
            "iter: 161, logloss: 0.326740, accuracy: 0.771200\n",
            "iter: 162, logloss: 0.326564, accuracy: 0.772000\n",
            "iter: 163, logloss: 0.326388, accuracy: 0.772800\n",
            "iter: 164, logloss: 0.326216, accuracy: 0.773200\n",
            "iter: 165, logloss: 0.326042, accuracy: 0.773600\n",
            "iter: 166, logloss: 0.325871, accuracy: 0.773800\n",
            "iter: 167, logloss: 0.325702, accuracy: 0.774200\n",
            "iter: 168, logloss: 0.325534, accuracy: 0.774200\n",
            "iter: 169, logloss: 0.325367, accuracy: 0.774400\n",
            "iter: 170, logloss: 0.325202, accuracy: 0.774800\n",
            "iter: 171, logloss: 0.325038, accuracy: 0.774800\n",
            "iter: 172, logloss: 0.324875, accuracy: 0.775000\n",
            "iter: 173, logloss: 0.324713, accuracy: 0.775000\n",
            "iter: 174, logloss: 0.324553, accuracy: 0.775000\n",
            "iter: 175, logloss: 0.324394, accuracy: 0.775400\n",
            "iter: 176, logloss: 0.324236, accuracy: 0.775600\n",
            "iter: 177, logloss: 0.324079, accuracy: 0.775800\n",
            "iter: 178, logloss: 0.323924, accuracy: 0.775600\n",
            "iter: 179, logloss: 0.323769, accuracy: 0.775600\n",
            "iter: 180, logloss: 0.323615, accuracy: 0.776400\n",
            "iter: 181, logloss: 0.323464, accuracy: 0.776600\n",
            "iter: 182, logloss: 0.323312, accuracy: 0.776600\n",
            "iter: 183, logloss: 0.323162, accuracy: 0.776800\n",
            "iter: 184, logloss: 0.323013, accuracy: 0.777200\n",
            "iter: 185, logloss: 0.322866, accuracy: 0.777800\n",
            "iter: 186, logloss: 0.322718, accuracy: 0.777600\n",
            "iter: 187, logloss: 0.322573, accuracy: 0.778000\n",
            "iter: 188, logloss: 0.322428, accuracy: 0.779000\n",
            "iter: 189, logloss: 0.322284, accuracy: 0.779200\n",
            "iter: 190, logloss: 0.322141, accuracy: 0.780200\n",
            "iter: 191, logloss: 0.321998, accuracy: 0.780400\n",
            "iter: 192, logloss: 0.321858, accuracy: 0.780400\n",
            "iter: 193, logloss: 0.321718, accuracy: 0.780400\n",
            "iter: 194, logloss: 0.321579, accuracy: 0.780600\n",
            "iter: 195, logloss: 0.321440, accuracy: 0.780600\n",
            "iter: 196, logloss: 0.321303, accuracy: 0.780600\n",
            "iter: 197, logloss: 0.321167, accuracy: 0.780800\n",
            "iter: 198, logloss: 0.321031, accuracy: 0.780800\n",
            "iter: 199, logloss: 0.320896, accuracy: 0.781200\n",
            "iter: 200, logloss: 0.320762, accuracy: 0.781200\n",
            "iter: 201, logloss: 0.320630, accuracy: 0.781200\n",
            "iter: 202, logloss: 0.320497, accuracy: 0.781200\n",
            "iter: 203, logloss: 0.320365, accuracy: 0.781200\n",
            "iter: 204, logloss: 0.320235, accuracy: 0.781600\n",
            "iter: 205, logloss: 0.320106, accuracy: 0.781800\n",
            "iter: 206, logloss: 0.319976, accuracy: 0.781800\n",
            "iter: 207, logloss: 0.319848, accuracy: 0.782200\n",
            "iter: 208, logloss: 0.319721, accuracy: 0.782400\n",
            "iter: 209, logloss: 0.319593, accuracy: 0.782800\n",
            "iter: 210, logloss: 0.319467, accuracy: 0.783400\n",
            "iter: 211, logloss: 0.319343, accuracy: 0.783600\n",
            "iter: 212, logloss: 0.319218, accuracy: 0.783800\n",
            "iter: 213, logloss: 0.319094, accuracy: 0.784400\n",
            "iter: 214, logloss: 0.318971, accuracy: 0.784400\n",
            "iter: 215, logloss: 0.318848, accuracy: 0.784600\n",
            "iter: 216, logloss: 0.318727, accuracy: 0.785200\n",
            "iter: 217, logloss: 0.318607, accuracy: 0.785800\n",
            "iter: 218, logloss: 0.318486, accuracy: 0.785800\n",
            "iter: 219, logloss: 0.318367, accuracy: 0.786000\n",
            "iter: 220, logloss: 0.318248, accuracy: 0.785800\n",
            "iter: 221, logloss: 0.318129, accuracy: 0.786000\n",
            "iter: 222, logloss: 0.318011, accuracy: 0.786400\n",
            "iter: 223, logloss: 0.317895, accuracy: 0.787000\n",
            "iter: 224, logloss: 0.317779, accuracy: 0.787400\n",
            "iter: 225, logloss: 0.317662, accuracy: 0.787800\n",
            "iter: 226, logloss: 0.317548, accuracy: 0.787800\n",
            "iter: 227, logloss: 0.317433, accuracy: 0.787800\n",
            "iter: 228, logloss: 0.317319, accuracy: 0.787800\n",
            "iter: 229, logloss: 0.317206, accuracy: 0.788200\n",
            "iter: 230, logloss: 0.317094, accuracy: 0.788400\n",
            "iter: 231, logloss: 0.316982, accuracy: 0.788400\n",
            "iter: 232, logloss: 0.316870, accuracy: 0.788400\n",
            "iter: 233, logloss: 0.316759, accuracy: 0.788400\n",
            "iter: 234, logloss: 0.316649, accuracy: 0.788600\n",
            "iter: 235, logloss: 0.316540, accuracy: 0.788600\n",
            "iter: 236, logloss: 0.316431, accuracy: 0.789000\n",
            "iter: 237, logloss: 0.316322, accuracy: 0.789400\n",
            "iter: 238, logloss: 0.316214, accuracy: 0.790000\n",
            "iter: 239, logloss: 0.316107, accuracy: 0.790200\n",
            "iter: 240, logloss: 0.316000, accuracy: 0.790400\n",
            "iter: 241, logloss: 0.315893, accuracy: 0.790400\n",
            "iter: 242, logloss: 0.315788, accuracy: 0.790600\n",
            "iter: 243, logloss: 0.315682, accuracy: 0.790800\n",
            "iter: 244, logloss: 0.315577, accuracy: 0.790800\n",
            "iter: 245, logloss: 0.315473, accuracy: 0.791400\n",
            "iter: 246, logloss: 0.315370, accuracy: 0.791600\n",
            "iter: 247, logloss: 0.315266, accuracy: 0.791600\n",
            "iter: 248, logloss: 0.315163, accuracy: 0.791600\n",
            "iter: 249, logloss: 0.315061, accuracy: 0.792000\n",
            "iter: 250, logloss: 0.314960, accuracy: 0.792000\n",
            "iter: 251, logloss: 0.314859, accuracy: 0.792000\n",
            "iter: 252, logloss: 0.314758, accuracy: 0.792000\n",
            "iter: 253, logloss: 0.314657, accuracy: 0.792200\n",
            "iter: 254, logloss: 0.314558, accuracy: 0.792200\n",
            "iter: 255, logloss: 0.314459, accuracy: 0.792200\n",
            "iter: 256, logloss: 0.314360, accuracy: 0.792400\n",
            "iter: 257, logloss: 0.314261, accuracy: 0.792400\n",
            "iter: 258, logloss: 0.314164, accuracy: 0.792600\n",
            "iter: 259, logloss: 0.314066, accuracy: 0.792800\n",
            "iter: 260, logloss: 0.313969, accuracy: 0.792600\n",
            "iter: 261, logloss: 0.313873, accuracy: 0.793000\n",
            "iter: 262, logloss: 0.313776, accuracy: 0.793400\n",
            "iter: 263, logloss: 0.313680, accuracy: 0.793600\n",
            "iter: 264, logloss: 0.313585, accuracy: 0.794000\n",
            "iter: 265, logloss: 0.313491, accuracy: 0.794200\n",
            "iter: 266, logloss: 0.313396, accuracy: 0.794400\n",
            "iter: 267, logloss: 0.313302, accuracy: 0.794600\n",
            "iter: 268, logloss: 0.313210, accuracy: 0.794600\n",
            "iter: 269, logloss: 0.313116, accuracy: 0.794800\n",
            "iter: 270, logloss: 0.313024, accuracy: 0.794600\n",
            "iter: 271, logloss: 0.312932, accuracy: 0.794600\n",
            "iter: 272, logloss: 0.312840, accuracy: 0.794400\n",
            "iter: 273, logloss: 0.312749, accuracy: 0.794400\n",
            "iter: 274, logloss: 0.312657, accuracy: 0.794600\n",
            "iter: 275, logloss: 0.312567, accuracy: 0.794800\n",
            "iter: 276, logloss: 0.312477, accuracy: 0.794600\n",
            "iter: 277, logloss: 0.312388, accuracy: 0.794400\n",
            "iter: 278, logloss: 0.312298, accuracy: 0.794800\n",
            "iter: 279, logloss: 0.312209, accuracy: 0.794800\n",
            "iter: 280, logloss: 0.312120, accuracy: 0.795000\n",
            "iter: 281, logloss: 0.312032, accuracy: 0.795400\n",
            "iter: 282, logloss: 0.311944, accuracy: 0.795600\n",
            "iter: 283, logloss: 0.311857, accuracy: 0.795800\n",
            "iter: 284, logloss: 0.311769, accuracy: 0.796000\n",
            "iter: 285, logloss: 0.311683, accuracy: 0.796200\n",
            "iter: 286, logloss: 0.311597, accuracy: 0.796400\n",
            "iter: 287, logloss: 0.311511, accuracy: 0.796600\n",
            "iter: 288, logloss: 0.311425, accuracy: 0.796800\n",
            "iter: 289, logloss: 0.311340, accuracy: 0.796800\n",
            "iter: 290, logloss: 0.311256, accuracy: 0.796800\n",
            "iter: 291, logloss: 0.311172, accuracy: 0.797200\n",
            "iter: 292, logloss: 0.311087, accuracy: 0.797200\n",
            "iter: 293, logloss: 0.311003, accuracy: 0.797400\n",
            "iter: 294, logloss: 0.310920, accuracy: 0.797600\n",
            "iter: 295, logloss: 0.310837, accuracy: 0.797600\n",
            "iter: 296, logloss: 0.310754, accuracy: 0.797600\n",
            "iter: 297, logloss: 0.310673, accuracy: 0.798000\n",
            "iter: 298, logloss: 0.310590, accuracy: 0.798200\n",
            "iter: 299, logloss: 0.310509, accuracy: 0.798600\n",
            "iter: 300, logloss: 0.310428, accuracy: 0.798600\n",
            "iter: 301, logloss: 0.310346, accuracy: 0.798800\n",
            "iter: 302, logloss: 0.310265, accuracy: 0.798800\n",
            "iter: 303, logloss: 0.310186, accuracy: 0.799000\n",
            "iter: 304, logloss: 0.310105, accuracy: 0.799200\n",
            "iter: 305, logloss: 0.310026, accuracy: 0.799600\n",
            "iter: 306, logloss: 0.309947, accuracy: 0.799600\n",
            "iter: 307, logloss: 0.309867, accuracy: 0.799800\n",
            "iter: 308, logloss: 0.309789, accuracy: 0.800000\n",
            "iter: 309, logloss: 0.309711, accuracy: 0.800200\n",
            "iter: 310, logloss: 0.309632, accuracy: 0.800800\n",
            "iter: 311, logloss: 0.309555, accuracy: 0.800800\n",
            "iter: 312, logloss: 0.309478, accuracy: 0.801000\n",
            "iter: 313, logloss: 0.309401, accuracy: 0.801200\n",
            "iter: 314, logloss: 0.309323, accuracy: 0.801600\n",
            "iter: 315, logloss: 0.309247, accuracy: 0.801600\n",
            "iter: 316, logloss: 0.309171, accuracy: 0.801600\n",
            "iter: 317, logloss: 0.309095, accuracy: 0.801800\n",
            "iter: 318, logloss: 0.309019, accuracy: 0.802000\n",
            "iter: 319, logloss: 0.308944, accuracy: 0.802000\n",
            "iter: 320, logloss: 0.308870, accuracy: 0.802000\n",
            "iter: 321, logloss: 0.308795, accuracy: 0.802000\n",
            "iter: 322, logloss: 0.308721, accuracy: 0.802200\n",
            "iter: 323, logloss: 0.308647, accuracy: 0.802400\n",
            "iter: 324, logloss: 0.308573, accuracy: 0.802800\n",
            "iter: 325, logloss: 0.308499, accuracy: 0.802800\n",
            "iter: 326, logloss: 0.308426, accuracy: 0.803000\n",
            "iter: 327, logloss: 0.308354, accuracy: 0.803000\n",
            "iter: 328, logloss: 0.308281, accuracy: 0.803200\n",
            "iter: 329, logloss: 0.308210, accuracy: 0.803600\n",
            "iter: 330, logloss: 0.308136, accuracy: 0.803600\n",
            "iter: 331, logloss: 0.308065, accuracy: 0.803800\n",
            "iter: 332, logloss: 0.307993, accuracy: 0.803800\n",
            "iter: 333, logloss: 0.307921, accuracy: 0.803600\n",
            "iter: 334, logloss: 0.307851, accuracy: 0.803800\n",
            "iter: 335, logloss: 0.307780, accuracy: 0.803800\n",
            "iter: 336, logloss: 0.307711, accuracy: 0.804200\n",
            "iter: 337, logloss: 0.307640, accuracy: 0.804200\n",
            "iter: 338, logloss: 0.307571, accuracy: 0.804200\n",
            "iter: 339, logloss: 0.307502, accuracy: 0.804600\n",
            "iter: 340, logloss: 0.307431, accuracy: 0.804800\n",
            "iter: 341, logloss: 0.307362, accuracy: 0.804800\n",
            "iter: 342, logloss: 0.307294, accuracy: 0.804800\n",
            "iter: 343, logloss: 0.307226, accuracy: 0.804800\n",
            "iter: 344, logloss: 0.307157, accuracy: 0.804800\n",
            "iter: 345, logloss: 0.307090, accuracy: 0.805000\n",
            "iter: 346, logloss: 0.307021, accuracy: 0.805200\n",
            "iter: 347, logloss: 0.306953, accuracy: 0.805200\n",
            "iter: 348, logloss: 0.306887, accuracy: 0.805600\n",
            "iter: 349, logloss: 0.306820, accuracy: 0.805800\n",
            "iter: 350, logloss: 0.306753, accuracy: 0.805800\n",
            "iter: 351, logloss: 0.306687, accuracy: 0.806000\n",
            "iter: 352, logloss: 0.306622, accuracy: 0.806000\n",
            "iter: 353, logloss: 0.306556, accuracy: 0.805800\n",
            "iter: 354, logloss: 0.306490, accuracy: 0.806000\n",
            "iter: 355, logloss: 0.306425, accuracy: 0.806000\n",
            "iter: 356, logloss: 0.306359, accuracy: 0.805800\n",
            "iter: 357, logloss: 0.306293, accuracy: 0.806000\n",
            "iter: 358, logloss: 0.306229, accuracy: 0.806200\n",
            "iter: 359, logloss: 0.306165, accuracy: 0.806200\n",
            "iter: 360, logloss: 0.306100, accuracy: 0.806200\n",
            "iter: 361, logloss: 0.306037, accuracy: 0.806200\n",
            "iter: 362, logloss: 0.305973, accuracy: 0.806200\n",
            "iter: 363, logloss: 0.305909, accuracy: 0.806400\n",
            "iter: 364, logloss: 0.305846, accuracy: 0.806600\n",
            "iter: 365, logloss: 0.305783, accuracy: 0.806600\n",
            "iter: 366, logloss: 0.305720, accuracy: 0.806800\n",
            "iter: 367, logloss: 0.305657, accuracy: 0.806800\n",
            "iter: 368, logloss: 0.305596, accuracy: 0.807000\n",
            "iter: 369, logloss: 0.305533, accuracy: 0.807000\n",
            "iter: 370, logloss: 0.305472, accuracy: 0.806800\n",
            "iter: 371, logloss: 0.305410, accuracy: 0.806800\n",
            "iter: 372, logloss: 0.305349, accuracy: 0.806800\n",
            "iter: 373, logloss: 0.305288, accuracy: 0.806800\n",
            "iter: 374, logloss: 0.305226, accuracy: 0.807000\n",
            "iter: 375, logloss: 0.305166, accuracy: 0.807200\n",
            "iter: 376, logloss: 0.305105, accuracy: 0.807200\n",
            "iter: 377, logloss: 0.305044, accuracy: 0.807400\n",
            "iter: 378, logloss: 0.304984, accuracy: 0.807200\n",
            "iter: 379, logloss: 0.304924, accuracy: 0.807200\n",
            "iter: 380, logloss: 0.304865, accuracy: 0.807200\n",
            "iter: 381, logloss: 0.304806, accuracy: 0.807200\n",
            "iter: 382, logloss: 0.304747, accuracy: 0.807400\n",
            "iter: 383, logloss: 0.304688, accuracy: 0.807600\n",
            "iter: 384, logloss: 0.304629, accuracy: 0.807600\n",
            "iter: 385, logloss: 0.304570, accuracy: 0.807800\n",
            "iter: 386, logloss: 0.304511, accuracy: 0.807800\n",
            "iter: 387, logloss: 0.304454, accuracy: 0.807800\n",
            "iter: 388, logloss: 0.304396, accuracy: 0.807600\n",
            "iter: 389, logloss: 0.304338, accuracy: 0.807600\n",
            "iter: 390, logloss: 0.304280, accuracy: 0.807600\n",
            "iter: 391, logloss: 0.304224, accuracy: 0.807600\n",
            "iter: 392, logloss: 0.304166, accuracy: 0.808000\n",
            "iter: 393, logloss: 0.304110, accuracy: 0.807800\n",
            "iter: 394, logloss: 0.304053, accuracy: 0.808200\n",
            "iter: 395, logloss: 0.303996, accuracy: 0.808200\n",
            "iter: 396, logloss: 0.303939, accuracy: 0.808400\n",
            "iter: 397, logloss: 0.303884, accuracy: 0.808800\n",
            "iter: 398, logloss: 0.303828, accuracy: 0.809000\n",
            "iter: 399, logloss: 0.303772, accuracy: 0.809000\n",
            "iter: 400, logloss: 0.303716, accuracy: 0.809200\n",
            "iter: 401, logloss: 0.303661, accuracy: 0.809200\n",
            "iter: 402, logloss: 0.303606, accuracy: 0.809200\n",
            "iter: 403, logloss: 0.303551, accuracy: 0.809200\n",
            "iter: 404, logloss: 0.303497, accuracy: 0.809600\n",
            "iter: 405, logloss: 0.303442, accuracy: 0.809800\n",
            "iter: 406, logloss: 0.303388, accuracy: 0.809800\n",
            "iter: 407, logloss: 0.303333, accuracy: 0.810000\n",
            "iter: 408, logloss: 0.303280, accuracy: 0.810000\n",
            "iter: 409, logloss: 0.303226, accuracy: 0.810000\n",
            "iter: 410, logloss: 0.303172, accuracy: 0.810200\n",
            "iter: 411, logloss: 0.303119, accuracy: 0.810200\n",
            "iter: 412, logloss: 0.303066, accuracy: 0.810200\n",
            "iter: 413, logloss: 0.303013, accuracy: 0.810400\n",
            "iter: 414, logloss: 0.302960, accuracy: 0.810400\n",
            "iter: 415, logloss: 0.302908, accuracy: 0.810400\n",
            "iter: 416, logloss: 0.302856, accuracy: 0.810400\n",
            "iter: 417, logloss: 0.302803, accuracy: 0.811000\n",
            "iter: 418, logloss: 0.302751, accuracy: 0.811200\n",
            "iter: 419, logloss: 0.302699, accuracy: 0.811400\n",
            "iter: 420, logloss: 0.302646, accuracy: 0.811800\n",
            "iter: 421, logloss: 0.302595, accuracy: 0.811600\n",
            "iter: 422, logloss: 0.302544, accuracy: 0.811600\n",
            "iter: 423, logloss: 0.302493, accuracy: 0.811600\n",
            "iter: 424, logloss: 0.302442, accuracy: 0.812000\n",
            "iter: 425, logloss: 0.302391, accuracy: 0.812200\n",
            "iter: 426, logloss: 0.302341, accuracy: 0.812400\n",
            "iter: 427, logloss: 0.302290, accuracy: 0.812400\n",
            "iter: 428, logloss: 0.302240, accuracy: 0.812400\n",
            "iter: 429, logloss: 0.302189, accuracy: 0.812400\n",
            "iter: 430, logloss: 0.302140, accuracy: 0.812200\n",
            "iter: 431, logloss: 0.302090, accuracy: 0.812200\n",
            "iter: 432, logloss: 0.302040, accuracy: 0.812400\n",
            "iter: 433, logloss: 0.301991, accuracy: 0.812200\n",
            "iter: 434, logloss: 0.301942, accuracy: 0.812200\n",
            "iter: 435, logloss: 0.301892, accuracy: 0.812400\n",
            "iter: 436, logloss: 0.301844, accuracy: 0.812400\n",
            "iter: 437, logloss: 0.301794, accuracy: 0.812400\n",
            "iter: 438, logloss: 0.301746, accuracy: 0.812400\n",
            "iter: 439, logloss: 0.301698, accuracy: 0.812400\n",
            "iter: 440, logloss: 0.301649, accuracy: 0.812800\n",
            "iter: 441, logloss: 0.301602, accuracy: 0.813200\n",
            "iter: 442, logloss: 0.301554, accuracy: 0.813400\n",
            "iter: 443, logloss: 0.301506, accuracy: 0.813400\n",
            "iter: 444, logloss: 0.301458, accuracy: 0.813200\n",
            "iter: 445, logloss: 0.301411, accuracy: 0.813600\n",
            "iter: 446, logloss: 0.301364, accuracy: 0.813600\n",
            "iter: 447, logloss: 0.301316, accuracy: 0.813600\n",
            "iter: 448, logloss: 0.301268, accuracy: 0.813600\n",
            "iter: 449, logloss: 0.301223, accuracy: 0.813600\n",
            "iter: 450, logloss: 0.301176, accuracy: 0.813600\n",
            "iter: 451, logloss: 0.301129, accuracy: 0.813800\n",
            "iter: 452, logloss: 0.301083, accuracy: 0.814000\n",
            "iter: 453, logloss: 0.301037, accuracy: 0.814200\n",
            "iter: 454, logloss: 0.300990, accuracy: 0.814400\n",
            "iter: 455, logloss: 0.300945, accuracy: 0.814200\n",
            "iter: 456, logloss: 0.300899, accuracy: 0.814800\n",
            "iter: 457, logloss: 0.300853, accuracy: 0.815000\n",
            "iter: 458, logloss: 0.300808, accuracy: 0.815000\n",
            "iter: 459, logloss: 0.300764, accuracy: 0.815200\n",
            "iter: 460, logloss: 0.300717, accuracy: 0.815200\n",
            "iter: 461, logloss: 0.300673, accuracy: 0.815200\n",
            "iter: 462, logloss: 0.300628, accuracy: 0.815600\n",
            "iter: 463, logloss: 0.300583, accuracy: 0.815600\n",
            "iter: 464, logloss: 0.300539, accuracy: 0.815600\n",
            "iter: 465, logloss: 0.300495, accuracy: 0.815800\n",
            "iter: 466, logloss: 0.300450, accuracy: 0.815600\n",
            "iter: 467, logloss: 0.300407, accuracy: 0.815600\n",
            "iter: 468, logloss: 0.300363, accuracy: 0.815600\n",
            "iter: 469, logloss: 0.300319, accuracy: 0.815600\n",
            "iter: 470, logloss: 0.300275, accuracy: 0.815600\n",
            "iter: 471, logloss: 0.300232, accuracy: 0.815600\n",
            "iter: 472, logloss: 0.300189, accuracy: 0.815600\n",
            "iter: 473, logloss: 0.300145, accuracy: 0.815600\n",
            "iter: 474, logloss: 0.300101, accuracy: 0.815800\n",
            "iter: 475, logloss: 0.300059, accuracy: 0.815800\n",
            "iter: 476, logloss: 0.300017, accuracy: 0.816000\n",
            "iter: 477, logloss: 0.299973, accuracy: 0.816000\n",
            "iter: 478, logloss: 0.299931, accuracy: 0.816200\n",
            "iter: 479, logloss: 0.299889, accuracy: 0.816200\n",
            "iter: 480, logloss: 0.299847, accuracy: 0.816400\n",
            "iter: 481, logloss: 0.299805, accuracy: 0.816600\n",
            "iter: 482, logloss: 0.299764, accuracy: 0.816800\n",
            "iter: 483, logloss: 0.299722, accuracy: 0.816600\n",
            "iter: 484, logloss: 0.299680, accuracy: 0.817000\n",
            "iter: 485, logloss: 0.299639, accuracy: 0.817200\n",
            "iter: 486, logloss: 0.299596, accuracy: 0.817200\n",
            "iter: 487, logloss: 0.299555, accuracy: 0.817400\n",
            "iter: 488, logloss: 0.299514, accuracy: 0.817200\n",
            "iter: 489, logloss: 0.299474, accuracy: 0.817200\n",
            "iter: 490, logloss: 0.299433, accuracy: 0.817200\n",
            "iter: 491, logloss: 0.299392, accuracy: 0.817400\n",
            "iter: 492, logloss: 0.299351, accuracy: 0.817800\n",
            "iter: 493, logloss: 0.299311, accuracy: 0.818000\n",
            "iter: 494, logloss: 0.299271, accuracy: 0.818000\n",
            "iter: 495, logloss: 0.299231, accuracy: 0.818400\n",
            "iter: 496, logloss: 0.299190, accuracy: 0.818600\n",
            "iter: 497, logloss: 0.299151, accuracy: 0.818800\n",
            "iter: 498, logloss: 0.299111, accuracy: 0.818800\n",
            "iter: 499, logloss: 0.299072, accuracy: 0.818800\n",
            "Duration (s): 0.687922\n",
            "final accuracy: 0.818800\n",
            "final weights: \n",
            "[\n",
            "-0.912856,\t1.054308;\n",
            "-0.200946,\t0.551628;\n",
            "-0.557171,\t0.623020;\n",
            "0.260683,\t-0.198299;\n",
            "-0.056367,\t-0.045094;\n",
            "0.354063,\t-0.289981;\n",
            "-0.693167,\t0.619710;\n",
            "0.636540,\t-0.112859;\n",
            "0.988858,\t-0.775986\n",
            "]\n",
            "CPU times: user 11.1 ms, sys: 6.54 ms, total: 17.6 ms\n",
            "Wall time: 1.43 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxIyxh0ClrIz"
      },
      "source": [
        "Let's plot the logloss and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "kR2kCNEIlpqQ",
        "outputId": "9fcb53ad-c91e-492d-e4d6-693e9c29874e"
      },
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "data = pd.read_csv('log.txt',sep=',',header=None)\n",
        "fig,ax = plt.subplots()\n",
        "ax.plot(data[0],label=\"logloss\")\n",
        "ax2=ax.twinx()\n",
        "ax2.plot([], [])\n",
        "ax2.plot(data[1],label=\"accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2z5QCx7zwmJ"
      },
      "source": [
        "Best accuracy: 0.818800\r\n",
        "Wall time: 1.43 s\r\n",
        "with learning rate = 0.1 and nb_epochs=500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17aUKkNJqTDZ"
      },
      "source": [
        "# Debugging\n",
        "Compile with debugging info on the host (`-g`) and device (`-G`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcfLGo1UrMq9"
      },
      "source": [
        "!nvcc -g -G -I /usr/local/cuda/samples/common/inc/ -L/usr/local/cuda/include -lcublas -lcusolver linear_classification.cu  read_csv.cu preprocess_data.cu evaluate_accuracy.cu fmatrix.cu classifier_math.cu cuda_stuff.cu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkuaGO10rRm9"
      },
      "source": [
        "Run the debugger cuda-gdb, stopping at the first error that is detected. Shows first the call stack on the GPU, the values of local variables, then the call stack on the host (thread 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ8nAtzGTRgH",
        "outputId": "a56cd07f-69a0-4815-c99b-484885b69c51"
      },
      "source": [
        "! printf \"set cuda memcheck on\\nset cuda api_failures stop\\ncatch throw\\nr\\nbt\\ninfo locals\\nthread 1\\nbt\\n\" > tmp.txt\n",
        "! cat tmp.txt\n",
        "! cuda-gdb -batch -x tmp.txt ./a.out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set cuda memcheck on\n",
            "set cuda api_failures stop\n",
            "catch throw\n",
            "r\n",
            "bt\n",
            "info locals\n",
            "thread 1\n",
            "bt\n",
            "Catchpoint 1 (throw)\n",
            "warning: Error disabling address space randomization: Operation not permitted\n",
            "[tcsetpgrp failed in terminal_inferior: Inappropriate ioctl for device]\n",
            "[tcsetpgrp failed in terminal_inferior: Inappropriate ioctl for device]\n",
            "[Thread debugging using libthread_db enabled]\n",
            "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
            "[tcsetpgrp failed in terminal_inferior: Inappropriate ioctl for device]\n",
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 17000 rows.\n",
            "Allocated memory for inputs: 17000 rows, 9 columns.\n",
            "Allocated memory for labels: 17000 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t-114.58\t-114.59\t-114.59\t-114.6\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t33.61\t34.83\t33.61\t34.83\t\n",
            "15\t19\t17\t14\t20\t29\t25\t41\t34\t46\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t2907\t812\t4789\t1497\t\n",
            "1283\t1901\t174\t337\t326\t236\t680\t168\t1175\t309\t\n",
            "1015\t1129\t333\t515\t624\t671\t1841\t375\t3134\t787\t\n",
            "472\t463\t117\t226\t262\t239\t633\t158\t1056\t271\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t2.6768\t1.7083\t2.1782\t2.1908\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "[tcsetpgrp failed in terminal_inferior: Inappropriate ioctl for device]\n",
            "[New Thread 0x7f4f7d6d8700 (LWP 436)]\n",
            "[New Thread 0x7f4f7ced7700 (LWP 437)]\n",
            "[tcsetpgrp failed in terminal_inferior: Inappropriate ioctl for device]\n",
            "initial accuracy: 0.340200\n",
            "iter: 0, logloss: 0.880316, accuracy: 0.340000\n",
            "iter: 1, logloss: 0.830006, accuracy: 0.340200\n",
            "iter: 2, logloss: 0.785198, accuracy: 0.342600\n",
            "iter: 3, logloss: 0.745490, accuracy: 0.370600\n",
            "iter: 4, logloss: 0.710397, accuracy: 0.434200\n",
            "iter: 5, logloss: 0.679411, accuracy: 0.529800\n",
            "iter: 6, logloss: 0.652040, accuracy: 0.622000\n",
            "iter: 7, logloss: 0.627811, accuracy: 0.691800\n",
            "iter: 8, logloss: 0.606301, accuracy: 0.730200\n",
            "iter: 9, logloss: 0.587134, accuracy: 0.743200\n",
            "iter: 10, logloss: 0.569993, accuracy: 0.743600\n",
            "iter: 11, logloss: 0.554603, accuracy: 0.743600\n",
            "iter: 12, logloss: 0.540726, accuracy: 0.739800\n",
            "iter: 13, logloss: 0.528171, accuracy: 0.735400\n",
            "iter: 14, logloss: 0.516767, accuracy: 0.733000\n",
            "iter: 15, logloss: 0.506375, accuracy: 0.730200\n",
            "iter: 16, logloss: 0.496874, accuracy: 0.728600\n",
            "iter: 17, logloss: 0.488162, accuracy: 0.727200\n",
            "iter: 18, logloss: 0.480151, accuracy: 0.725800\n",
            "iter: 19, logloss: 0.472766, accuracy: 0.724800\n",
            "iter: 20, logloss: 0.465936, accuracy: 0.723800\n",
            "iter: 21, logloss: 0.459610, accuracy: 0.723600\n",
            "iter: 22, logloss: 0.453735, accuracy: 0.723800\n",
            "iter: 23, logloss: 0.448267, accuracy: 0.723600\n",
            "iter: 24, logloss: 0.443169, accuracy: 0.723600\n",
            "iter: 25, logloss: 0.438407, accuracy: 0.723800\n",
            "iter: 26, logloss: 0.433951, accuracy: 0.723600\n",
            "iter: 27, logloss: 0.429771, accuracy: 0.723800\n",
            "iter: 28, logloss: 0.425848, accuracy: 0.724000\n",
            "iter: 29, logloss: 0.422157, accuracy: 0.724200\n",
            "iter: 30, logloss: 0.418679, accuracy: 0.724400\n",
            "iter: 31, logloss: 0.415401, accuracy: 0.724600\n",
            "iter: 32, logloss: 0.412301, accuracy: 0.724800\n",
            "iter: 33, logloss: 0.409372, accuracy: 0.724800\n",
            "iter: 34, logloss: 0.406597, accuracy: 0.725000\n",
            "iter: 35, logloss: 0.403964, accuracy: 0.725400\n",
            "iter: 36, logloss: 0.401464, accuracy: 0.726400\n",
            "iter: 37, logloss: 0.399088, accuracy: 0.727000\n",
            "iter: 38, logloss: 0.396828, accuracy: 0.727400\n",
            "iter: 39, logloss: 0.394675, accuracy: 0.727800\n",
            "iter: 40, logloss: 0.392620, accuracy: 0.727800\n",
            "iter: 41, logloss: 0.390660, accuracy: 0.728800\n",
            "iter: 42, logloss: 0.388785, accuracy: 0.729400\n",
            "iter: 43, logloss: 0.386994, accuracy: 0.730000\n",
            "iter: 44, logloss: 0.385277, accuracy: 0.730000\n",
            "iter: 45, logloss: 0.383634, accuracy: 0.730400\n",
            "iter: 46, logloss: 0.382058, accuracy: 0.730600\n",
            "iter: 47, logloss: 0.380545, accuracy: 0.730600\n",
            "iter: 48, logloss: 0.379091, accuracy: 0.731600\n",
            "iter: 49, logloss: 0.377695, accuracy: 0.732200\n",
            "iter: 50, logloss: 0.376349, accuracy: 0.733000\n",
            "iter: 51, logloss: 0.375054, accuracy: 0.733200\n",
            "iter: 52, logloss: 0.373807, accuracy: 0.733600\n",
            "iter: 53, logloss: 0.372606, accuracy: 0.733800\n",
            "iter: 54, logloss: 0.371445, accuracy: 0.734200\n",
            "iter: 55, logloss: 0.370325, accuracy: 0.734600\n",
            "iter: 56, logloss: 0.369244, accuracy: 0.735400\n",
            "iter: 57, logloss: 0.368197, accuracy: 0.736200\n",
            "iter: 58, logloss: 0.367185, accuracy: 0.736800\n",
            "iter: 59, logloss: 0.366205, accuracy: 0.737400\n",
            "iter: 60, logloss: 0.365257, accuracy: 0.737800\n",
            "iter: 61, logloss: 0.364338, accuracy: 0.738400\n",
            "iter: 62, logloss: 0.363446, accuracy: 0.738800\n",
            "iter: 63, logloss: 0.362582, accuracy: 0.738800\n",
            "iter: 64, logloss: 0.361742, accuracy: 0.739200\n",
            "iter: 65, logloss: 0.360929, accuracy: 0.739200\n",
            "iter: 66, logloss: 0.360137, accuracy: 0.740400\n",
            "iter: 67, logloss: 0.359368, accuracy: 0.740800\n",
            "iter: 68, logloss: 0.358621, accuracy: 0.741400\n",
            "iter: 69, logloss: 0.357892, accuracy: 0.742000\n",
            "iter: 70, logloss: 0.357182, accuracy: 0.742400\n",
            "iter: 71, logloss: 0.356493, accuracy: 0.742800\n",
            "iter: 72, logloss: 0.355821, accuracy: 0.743800\n",
            "iter: 73, logloss: 0.355166, accuracy: 0.744200\n",
            "iter: 74, logloss: 0.354528, accuracy: 0.744600\n",
            "iter: 75, logloss: 0.353904, accuracy: 0.744800\n",
            "iter: 76, logloss: 0.353296, accuracy: 0.745400\n",
            "iter: 77, logloss: 0.352704, accuracy: 0.745600\n",
            "iter: 78, logloss: 0.352125, accuracy: 0.746000\n",
            "iter: 79, logloss: 0.351558, accuracy: 0.746200\n",
            "iter: 80, logloss: 0.351005, accuracy: 0.746400\n",
            "iter: 81, logloss: 0.350463, accuracy: 0.746800\n",
            "iter: 82, logloss: 0.349935, accuracy: 0.747200\n",
            "iter: 83, logloss: 0.349419, accuracy: 0.747200\n",
            "iter: 84, logloss: 0.348912, accuracy: 0.747400\n",
            "iter: 85, logloss: 0.348416, accuracy: 0.747400\n",
            "iter: 86, logloss: 0.347931, accuracy: 0.747600\n",
            "iter: 87, logloss: 0.347456, accuracy: 0.747600\n",
            "iter: 88, logloss: 0.346990, accuracy: 0.748200\n",
            "iter: 89, logloss: 0.346533, accuracy: 0.748800\n",
            "iter: 90, logloss: 0.346086, accuracy: 0.749200\n",
            "iter: 91, logloss: 0.345648, accuracy: 0.749200\n",
            "iter: 92, logloss: 0.345217, accuracy: 0.749800\n",
            "iter: 93, logloss: 0.344795, accuracy: 0.750400\n",
            "iter: 94, logloss: 0.344381, accuracy: 0.750600\n",
            "iter: 95, logloss: 0.343974, accuracy: 0.751000\n",
            "iter: 96, logloss: 0.343573, accuracy: 0.751200\n",
            "iter: 97, logloss: 0.343181, accuracy: 0.751400\n",
            "iter: 98, logloss: 0.342796, accuracy: 0.751400\n",
            "iter: 99, logloss: 0.342417, accuracy: 0.751600\n",
            "iter: 100, logloss: 0.342044, accuracy: 0.752000\n",
            "iter: 101, logloss: 0.341678, accuracy: 0.752800\n",
            "iter: 102, logloss: 0.341319, accuracy: 0.752800\n",
            "iter: 103, logloss: 0.340965, accuracy: 0.753000\n",
            "iter: 104, logloss: 0.340616, accuracy: 0.753600\n",
            "iter: 105, logloss: 0.340273, accuracy: 0.753800\n",
            "iter: 106, logloss: 0.339935, accuracy: 0.754200\n",
            "iter: 107, logloss: 0.339603, accuracy: 0.755000\n",
            "iter: 108, logloss: 0.339276, accuracy: 0.755000\n",
            "iter: 109, logloss: 0.338955, accuracy: 0.755400\n",
            "iter: 110, logloss: 0.338637, accuracy: 0.755800\n",
            "iter: 111, logloss: 0.338325, accuracy: 0.756000\n",
            "iter: 112, logloss: 0.338018, accuracy: 0.756200\n",
            "iter: 113, logloss: 0.337713, accuracy: 0.756800\n",
            "iter: 114, logloss: 0.337415, accuracy: 0.757600\n",
            "iter: 115, logloss: 0.337120, accuracy: 0.757600\n",
            "iter: 116, logloss: 0.336828, accuracy: 0.757800\n",
            "iter: 117, logloss: 0.336541, accuracy: 0.758600\n",
            "iter: 118, logloss: 0.336259, accuracy: 0.758600\n",
            "iter: 119, logloss: 0.335979, accuracy: 0.759200\n",
            "iter: 120, logloss: 0.335703, accuracy: 0.759400\n",
            "iter: 121, logloss: 0.335433, accuracy: 0.759600\n",
            "iter: 122, logloss: 0.335164, accuracy: 0.760200\n",
            "iter: 123, logloss: 0.334898, accuracy: 0.760600\n",
            "iter: 124, logloss: 0.334637, accuracy: 0.760600\n",
            "iter: 125, logloss: 0.334379, accuracy: 0.760600\n",
            "iter: 126, logloss: 0.334123, accuracy: 0.761000\n",
            "iter: 127, logloss: 0.333872, accuracy: 0.761600\n",
            "iter: 128, logloss: 0.333622, accuracy: 0.761800\n",
            "iter: 129, logloss: 0.333377, accuracy: 0.762200\n",
            "iter: 130, logloss: 0.333133, accuracy: 0.762400\n",
            "iter: 131, logloss: 0.332893, accuracy: 0.762800\n",
            "iter: 132, logloss: 0.332656, accuracy: 0.763400\n",
            "iter: 133, logloss: 0.332421, accuracy: 0.764200\n",
            "iter: 134, logloss: 0.332188, accuracy: 0.764400\n",
            "iter: 135, logloss: 0.331958, accuracy: 0.764600\n",
            "iter: 136, logloss: 0.331732, accuracy: 0.764800\n",
            "iter: 137, logloss: 0.331507, accuracy: 0.765000\n",
            "iter: 138, logloss: 0.331284, accuracy: 0.765200\n",
            "iter: 139, logloss: 0.331065, accuracy: 0.765400\n",
            "iter: 140, logloss: 0.330847, accuracy: 0.765400\n",
            "iter: 141, logloss: 0.330632, accuracy: 0.765800\n",
            "iter: 142, logloss: 0.330418, accuracy: 0.766200\n",
            "iter: 143, logloss: 0.330208, accuracy: 0.766600\n",
            "iter: 144, logloss: 0.330000, accuracy: 0.767400\n",
            "iter: 145, logloss: 0.329793, accuracy: 0.767600\n",
            "iter: 146, logloss: 0.329589, accuracy: 0.767800\n",
            "iter: 147, logloss: 0.329386, accuracy: 0.768200\n",
            "iter: 148, logloss: 0.329185, accuracy: 0.768400\n",
            "iter: 149, logloss: 0.328987, accuracy: 0.768800\n",
            "iter: 150, logloss: 0.328789, accuracy: 0.769000\n",
            "iter: 151, logloss: 0.328595, accuracy: 0.769200\n",
            "iter: 152, logloss: 0.328400, accuracy: 0.769200\n",
            "iter: 153, logloss: 0.328210, accuracy: 0.769600\n",
            "iter: 154, logloss: 0.328020, accuracy: 0.769600\n",
            "iter: 155, logloss: 0.327833, accuracy: 0.770000\n",
            "iter: 156, logloss: 0.327646, accuracy: 0.770200\n",
            "iter: 157, logloss: 0.327462, accuracy: 0.770600\n",
            "iter: 158, logloss: 0.327280, accuracy: 0.770800\n",
            "iter: 159, logloss: 0.327099, accuracy: 0.770800\n",
            "iter: 160, logloss: 0.326919, accuracy: 0.771000\n",
            "iter: 161, logloss: 0.326740, accuracy: 0.771200\n",
            "iter: 162, logloss: 0.326564, accuracy: 0.772000\n",
            "iter: 163, logloss: 0.326388, accuracy: 0.772800\n",
            "iter: 164, logloss: 0.326216, accuracy: 0.773200\n",
            "iter: 165, logloss: 0.326042, accuracy: 0.773600\n",
            "iter: 166, logloss: 0.325871, accuracy: 0.773800\n",
            "iter: 167, logloss: 0.325702, accuracy: 0.774200\n",
            "iter: 168, logloss: 0.325534, accuracy: 0.774200\n",
            "iter: 169, logloss: 0.325367, accuracy: 0.774400\n",
            "iter: 170, logloss: 0.325202, accuracy: 0.774800\n",
            "iter: 171, logloss: 0.325038, accuracy: 0.774800\n",
            "iter: 172, logloss: 0.324875, accuracy: 0.775000\n",
            "iter: 173, logloss: 0.324713, accuracy: 0.775000\n",
            "iter: 174, logloss: 0.324553, accuracy: 0.775000\n",
            "iter: 175, logloss: 0.324394, accuracy: 0.775400\n",
            "iter: 176, logloss: 0.324236, accuracy: 0.775600\n",
            "iter: 177, logloss: 0.324079, accuracy: 0.775800\n",
            "iter: 178, logloss: 0.323924, accuracy: 0.775600\n",
            "iter: 179, logloss: 0.323769, accuracy: 0.775600\n",
            "iter: 180, logloss: 0.323615, accuracy: 0.776400\n",
            "iter: 181, logloss: 0.323464, accuracy: 0.776600\n",
            "iter: 182, logloss: 0.323312, accuracy: 0.776600\n",
            "iter: 183, logloss: 0.323162, accuracy: 0.776800\n",
            "iter: 184, logloss: 0.323013, accuracy: 0.777200\n",
            "iter: 185, logloss: 0.322866, accuracy: 0.777800\n",
            "iter: 186, logloss: 0.322718, accuracy: 0.777600\n",
            "iter: 187, logloss: 0.322573, accuracy: 0.778000\n",
            "iter: 188, logloss: 0.322428, accuracy: 0.779000\n",
            "iter: 189, logloss: 0.322284, accuracy: 0.779200\n",
            "iter: 190, logloss: 0.322141, accuracy: 0.780200\n",
            "iter: 191, logloss: 0.321998, accuracy: 0.780400\n",
            "iter: 192, logloss: 0.321858, accuracy: 0.780400\n",
            "iter: 193, logloss: 0.321718, accuracy: 0.780400\n",
            "iter: 194, logloss: 0.321579, accuracy: 0.780600\n",
            "iter: 195, logloss: 0.321440, accuracy: 0.780600\n",
            "iter: 196, logloss: 0.321303, accuracy: 0.780600\n",
            "iter: 197, logloss: 0.321167, accuracy: 0.780800\n",
            "iter: 198, logloss: 0.321031, accuracy: 0.780800\n",
            "iter: 199, logloss: 0.320896, accuracy: 0.781200\n",
            "iter: 200, logloss: 0.320762, accuracy: 0.781200\n",
            "iter: 201, logloss: 0.320630, accuracy: 0.781200\n",
            "iter: 202, logloss: 0.320497, accuracy: 0.781200\n",
            "iter: 203, logloss: 0.320365, accuracy: 0.781200\n",
            "iter: 204, logloss: 0.320235, accuracy: 0.781600\n",
            "iter: 205, logloss: 0.320106, accuracy: 0.781800\n",
            "iter: 206, logloss: 0.319976, accuracy: 0.781800\n",
            "iter: 207, logloss: 0.319848, accuracy: 0.782200\n",
            "iter: 208, logloss: 0.319721, accuracy: 0.782400\n",
            "iter: 209, logloss: 0.319593, accuracy: 0.782800\n",
            "iter: 210, logloss: 0.319467, accuracy: 0.783400\n",
            "iter: 211, logloss: 0.319343, accuracy: 0.783600\n",
            "iter: 212, logloss: 0.319218, accuracy: 0.783800\n",
            "iter: 213, logloss: 0.319094, accuracy: 0.784400\n",
            "iter: 214, logloss: 0.318971, accuracy: 0.784400\n",
            "iter: 215, logloss: 0.318848, accuracy: 0.784600\n",
            "iter: 216, logloss: 0.318727, accuracy: 0.785200\n",
            "iter: 217, logloss: 0.318607, accuracy: 0.785800\n",
            "iter: 218, logloss: 0.318486, accuracy: 0.785800\n",
            "iter: 219, logloss: 0.318367, accuracy: 0.786000\n",
            "iter: 220, logloss: 0.318248, accuracy: 0.785800\n",
            "iter: 221, logloss: 0.318129, accuracy: 0.786000\n",
            "iter: 222, logloss: 0.318011, accuracy: 0.786400\n",
            "iter: 223, logloss: 0.317895, accuracy: 0.787000\n",
            "iter: 224, logloss: 0.317779, accuracy: 0.787400\n",
            "iter: 225, logloss: 0.317662, accuracy: 0.787800\n",
            "iter: 226, logloss: 0.317548, accuracy: 0.787800\n",
            "iter: 227, logloss: 0.317433, accuracy: 0.787800\n",
            "iter: 228, logloss: 0.317319, accuracy: 0.787800\n",
            "iter: 229, logloss: 0.317206, accuracy: 0.788200\n",
            "iter: 230, logloss: 0.317094, accuracy: 0.788400\n",
            "iter: 231, logloss: 0.316982, accuracy: 0.788400\n",
            "iter: 232, logloss: 0.316870, accuracy: 0.788400\n",
            "iter: 233, logloss: 0.316759, accuracy: 0.788400\n",
            "iter: 234, logloss: 0.316649, accuracy: 0.788600\n",
            "iter: 235, logloss: 0.316540, accuracy: 0.788600\n",
            "iter: 236, logloss: 0.316431, accuracy: 0.789000\n",
            "iter: 237, logloss: 0.316322, accuracy: 0.789400\n",
            "iter: 238, logloss: 0.316214, accuracy: 0.790000\n",
            "iter: 239, logloss: 0.316107, accuracy: 0.790200\n",
            "iter: 240, logloss: 0.316000, accuracy: 0.790400\n",
            "iter: 241, logloss: 0.315893, accuracy: 0.790400\n",
            "iter: 242, logloss: 0.315788, accuracy: 0.790600\n",
            "iter: 243, logloss: 0.315682, accuracy: 0.790800\n",
            "iter: 244, logloss: 0.315577, accuracy: 0.790800\n",
            "iter: 245, logloss: 0.315473, accuracy: 0.791400\n",
            "iter: 246, logloss: 0.315370, accuracy: 0.791600\n",
            "iter: 247, logloss: 0.315266, accuracy: 0.791600\n",
            "iter: 248, logloss: 0.315163, accuracy: 0.791600\n",
            "iter: 249, logloss: 0.315061, accuracy: 0.792000\n",
            "iter: 250, logloss: 0.314960, accuracy: 0.792000\n",
            "iter: 251, logloss: 0.314859, accuracy: 0.792000\n",
            "iter: 252, logloss: 0.314758, accuracy: 0.792000\n",
            "iter: 253, logloss: 0.314657, accuracy: 0.792200\n",
            "iter: 254, logloss: 0.314558, accuracy: 0.792200\n",
            "iter: 255, logloss: 0.314459, accuracy: 0.792200\n",
            "iter: 256, logloss: 0.314360, accuracy: 0.792400\n",
            "iter: 257, logloss: 0.314261, accuracy: 0.792400\n",
            "iter: 258, logloss: 0.314164, accuracy: 0.792600\n",
            "iter: 259, logloss: 0.314066, accuracy: 0.792800\n",
            "iter: 260, logloss: 0.313969, accuracy: 0.792600\n",
            "iter: 261, logloss: 0.313873, accuracy: 0.793000\n",
            "iter: 262, logloss: 0.313776, accuracy: 0.793400\n",
            "iter: 263, logloss: 0.313680, accuracy: 0.793600\n",
            "iter: 264, logloss: 0.313585, accuracy: 0.794000\n",
            "iter: 265, logloss: 0.313491, accuracy: 0.794200\n",
            "iter: 266, logloss: 0.313396, accuracy: 0.794400\n",
            "iter: 267, logloss: 0.313302, accuracy: 0.794600\n",
            "iter: 268, logloss: 0.313210, accuracy: 0.794600\n",
            "iter: 269, logloss: 0.313116, accuracy: 0.794800\n",
            "iter: 270, logloss: 0.313024, accuracy: 0.794600\n",
            "iter: 271, logloss: 0.312932, accuracy: 0.794600\n",
            "iter: 272, logloss: 0.312840, accuracy: 0.794400\n",
            "iter: 273, logloss: 0.312749, accuracy: 0.794400\n",
            "iter: 274, logloss: 0.312657, accuracy: 0.794600\n",
            "iter: 275, logloss: 0.312567, accuracy: 0.794800\n",
            "iter: 276, logloss: 0.312477, accuracy: 0.794600\n",
            "iter: 277, logloss: 0.312388, accuracy: 0.794400\n",
            "iter: 278, logloss: 0.312298, accuracy: 0.794800\n",
            "iter: 279, logloss: 0.312209, accuracy: 0.794800\n",
            "iter: 280, logloss: 0.312120, accuracy: 0.795000\n",
            "iter: 281, logloss: 0.312032, accuracy: 0.795400\n",
            "iter: 282, logloss: 0.311944, accuracy: 0.795600\n",
            "iter: 283, logloss: 0.311857, accuracy: 0.795800\n",
            "iter: 284, logloss: 0.311769, accuracy: 0.796000\n",
            "iter: 285, logloss: 0.311683, accuracy: 0.796200\n",
            "iter: 286, logloss: 0.311597, accuracy: 0.796400\n",
            "iter: 287, logloss: 0.311511, accuracy: 0.796600\n",
            "iter: 288, logloss: 0.311425, accuracy: 0.796800\n",
            "iter: 289, logloss: 0.311340, accuracy: 0.796800\n",
            "iter: 290, logloss: 0.311256, accuracy: 0.796800\n",
            "iter: 291, logloss: 0.311172, accuracy: 0.797200\n",
            "iter: 292, logloss: 0.311087, accuracy: 0.797200\n",
            "iter: 293, logloss: 0.311003, accuracy: 0.797400\n",
            "iter: 294, logloss: 0.310920, accuracy: 0.797600\n",
            "iter: 295, logloss: 0.310837, accuracy: 0.797600\n",
            "iter: 296, logloss: 0.310754, accuracy: 0.797600\n",
            "iter: 297, logloss: 0.310673, accuracy: 0.798000\n",
            "iter: 298, logloss: 0.310590, accuracy: 0.798200\n",
            "iter: 299, logloss: 0.310509, accuracy: 0.798600\n",
            "iter: 300, logloss: 0.310428, accuracy: 0.798600\n",
            "iter: 301, logloss: 0.310346, accuracy: 0.798800\n",
            "iter: 302, logloss: 0.310265, accuracy: 0.798800\n",
            "iter: 303, logloss: 0.310186, accuracy: 0.799000\n",
            "iter: 304, logloss: 0.310105, accuracy: 0.799200\n",
            "iter: 305, logloss: 0.310026, accuracy: 0.799600\n",
            "iter: 306, logloss: 0.309947, accuracy: 0.799600\n",
            "iter: 307, logloss: 0.309867, accuracy: 0.799800\n",
            "iter: 308, logloss: 0.309789, accuracy: 0.800000\n",
            "iter: 309, logloss: 0.309711, accuracy: 0.800200\n",
            "iter: 310, logloss: 0.309632, accuracy: 0.800800\n",
            "iter: 311, logloss: 0.309555, accuracy: 0.800800\n",
            "iter: 312, logloss: 0.309478, accuracy: 0.801000\n",
            "iter: 313, logloss: 0.309401, accuracy: 0.801200\n",
            "iter: 314, logloss: 0.309323, accuracy: 0.801600\n",
            "iter: 315, logloss: 0.309247, accuracy: 0.801600\n",
            "iter: 316, logloss: 0.309171, accuracy: 0.801600\n",
            "iter: 317, logloss: 0.309095, accuracy: 0.801800\n",
            "iter: 318, logloss: 0.309019, accuracy: 0.802000\n",
            "iter: 319, logloss: 0.308944, accuracy: 0.802000\n",
            "iter: 320, logloss: 0.308870, accuracy: 0.802000\n",
            "iter: 321, logloss: 0.308795, accuracy: 0.802000\n",
            "iter: 322, logloss: 0.308721, accuracy: 0.802200\n",
            "iter: 323, logloss: 0.308647, accuracy: 0.802400\n",
            "iter: 324, logloss: 0.308573, accuracy: 0.802800\n",
            "iter: 325, logloss: 0.308499, accuracy: 0.802800\n",
            "iter: 326, logloss: 0.308426, accuracy: 0.803000\n",
            "iter: 327, logloss: 0.308354, accuracy: 0.803000\n",
            "iter: 328, logloss: 0.308281, accuracy: 0.803200\n",
            "iter: 329, logloss: 0.308210, accuracy: 0.803600\n",
            "iter: 330, logloss: 0.308136, accuracy: 0.803600\n",
            "iter: 331, logloss: 0.308065, accuracy: 0.803800\n",
            "iter: 332, logloss: 0.307993, accuracy: 0.803800\n",
            "iter: 333, logloss: 0.307921, accuracy: 0.803600\n",
            "iter: 334, logloss: 0.307851, accuracy: 0.803800\n",
            "iter: 335, logloss: 0.307780, accuracy: 0.803800\n",
            "iter: 336, logloss: 0.307711, accuracy: 0.804200\n",
            "iter: 337, logloss: 0.307640, accuracy: 0.804200\n",
            "iter: 338, logloss: 0.307571, accuracy: 0.804200\n",
            "iter: 339, logloss: 0.307502, accuracy: 0.804600\n",
            "iter: 340, logloss: 0.307431, accuracy: 0.804800\n",
            "iter: 341, logloss: 0.307362, accuracy: 0.804800\n",
            "iter: 342, logloss: 0.307294, accuracy: 0.804800\n",
            "iter: 343, logloss: 0.307226, accuracy: 0.804800\n",
            "iter: 344, logloss: 0.307157, accuracy: 0.804800\n",
            "iter: 345, logloss: 0.307090, accuracy: 0.805000\n",
            "iter: 346, logloss: 0.307021, accuracy: 0.805200\n",
            "iter: 347, logloss: 0.306953, accuracy: 0.805200\n",
            "iter: 348, logloss: 0.306887, accuracy: 0.805600\n",
            "iter: 349, logloss: 0.306820, accuracy: 0.805800\n",
            "iter: 350, logloss: 0.306753, accuracy: 0.805800\n",
            "iter: 351, logloss: 0.306687, accuracy: 0.806000\n",
            "iter: 352, logloss: 0.306622, accuracy: 0.806000\n",
            "iter: 353, logloss: 0.306556, accuracy: 0.805800\n",
            "iter: 354, logloss: 0.306490, accuracy: 0.806000\n",
            "iter: 355, logloss: 0.306425, accuracy: 0.806000\n",
            "iter: 356, logloss: 0.306359, accuracy: 0.805800\n",
            "iter: 357, logloss: 0.306293, accuracy: 0.806000\n",
            "iter: 358, logloss: 0.306229, accuracy: 0.806200\n",
            "iter: 359, logloss: 0.306165, accuracy: 0.806200\n",
            "iter: 360, logloss: 0.306100, accuracy: 0.806200\n",
            "iter: 361, logloss: 0.306037, accuracy: 0.806200\n",
            "iter: 362, logloss: 0.305973, accuracy: 0.806200\n",
            "iter: 363, logloss: 0.305909, accuracy: 0.806400\n",
            "iter: 364, logloss: 0.305846, accuracy: 0.806600\n",
            "iter: 365, logloss: 0.305783, accuracy: 0.806600\n",
            "iter: 366, logloss: 0.305720, accuracy: 0.806800\n",
            "iter: 367, logloss: 0.305657, accuracy: 0.806800\n",
            "iter: 368, logloss: 0.305596, accuracy: 0.807000\n",
            "iter: 369, logloss: 0.305533, accuracy: 0.807000\n",
            "iter: 370, logloss: 0.305472, accuracy: 0.806800\n",
            "iter: 371, logloss: 0.305410, accuracy: 0.806800\n",
            "iter: 372, logloss: 0.305349, accuracy: 0.806800\n",
            "iter: 373, logloss: 0.305288, accuracy: 0.806800\n",
            "iter: 374, logloss: 0.305226, accuracy: 0.807000\n",
            "iter: 375, logloss: 0.305166, accuracy: 0.807200\n",
            "iter: 376, logloss: 0.305105, accuracy: 0.807200\n",
            "iter: 377, logloss: 0.305044, accuracy: 0.807400\n",
            "iter: 378, logloss: 0.304984, accuracy: 0.807200\n",
            "iter: 379, logloss: 0.304924, accuracy: 0.807200\n",
            "iter: 380, logloss: 0.304865, accuracy: 0.807200\n",
            "iter: 381, logloss: 0.304806, accuracy: 0.807200\n",
            "iter: 382, logloss: 0.304747, accuracy: 0.807400\n",
            "iter: 383, logloss: 0.304688, accuracy: 0.807600\n",
            "iter: 384, logloss: 0.304629, accuracy: 0.807600\n",
            "iter: 385, logloss: 0.304570, accuracy: 0.807800\n",
            "iter: 386, logloss: 0.304511, accuracy: 0.807800\n",
            "iter: 387, logloss: 0.304454, accuracy: 0.807800\n",
            "iter: 388, logloss: 0.304396, accuracy: 0.807600\n",
            "iter: 389, logloss: 0.304338, accuracy: 0.807600\n",
            "iter: 390, logloss: 0.304280, accuracy: 0.807600\n",
            "iter: 391, logloss: 0.304224, accuracy: 0.807600\n",
            "iter: 392, logloss: 0.304166, accuracy: 0.808000\n",
            "iter: 393, logloss: 0.304110, accuracy: 0.807800\n",
            "iter: 394, logloss: 0.304053, accuracy: 0.808200\n",
            "iter: 395, logloss: 0.303996, accuracy: 0.808200\n",
            "iter: 396, logloss: 0.303939, accuracy: 0.808400\n",
            "iter: 397, logloss: 0.303884, accuracy: 0.808800\n",
            "iter: 398, logloss: 0.303828, accuracy: 0.809000\n",
            "iter: 399, logloss: 0.303772, accuracy: 0.809000\n",
            "iter: 400, logloss: 0.303716, accuracy: 0.809200\n",
            "iter: 401, logloss: 0.303661, accuracy: 0.809200\n",
            "iter: 402, logloss: 0.303606, accuracy: 0.809200\n",
            "iter: 403, logloss: 0.303551, accuracy: 0.809200\n",
            "iter: 404, logloss: 0.303497, accuracy: 0.809600\n",
            "iter: 405, logloss: 0.303442, accuracy: 0.809800\n",
            "iter: 406, logloss: 0.303388, accuracy: 0.809800\n",
            "iter: 407, logloss: 0.303333, accuracy: 0.810000\n",
            "iter: 408, logloss: 0.303280, accuracy: 0.810000\n",
            "iter: 409, logloss: 0.303226, accuracy: 0.810000\n",
            "iter: 410, logloss: 0.303172, accuracy: 0.810200\n",
            "iter: 411, logloss: 0.303119, accuracy: 0.810200\n",
            "iter: 412, logloss: 0.303066, accuracy: 0.810200\n",
            "iter: 413, logloss: 0.303013, accuracy: 0.810400\n",
            "iter: 414, logloss: 0.302960, accuracy: 0.810400\n",
            "iter: 415, logloss: 0.302908, accuracy: 0.810400\n",
            "iter: 416, logloss: 0.302856, accuracy: 0.810400\n",
            "iter: 417, logloss: 0.302803, accuracy: 0.811000\n",
            "iter: 418, logloss: 0.302751, accuracy: 0.811200\n",
            "iter: 419, logloss: 0.302699, accuracy: 0.811400\n",
            "iter: 420, logloss: 0.302646, accuracy: 0.811800\n",
            "iter: 421, logloss: 0.302595, accuracy: 0.811600\n",
            "iter: 422, logloss: 0.302544, accuracy: 0.811600\n",
            "iter: 423, logloss: 0.302493, accuracy: 0.811600\n",
            "iter: 424, logloss: 0.302442, accuracy: 0.812000\n",
            "iter: 425, logloss: 0.302391, accuracy: 0.812200\n",
            "iter: 426, logloss: 0.302341, accuracy: 0.812400\n",
            "iter: 427, logloss: 0.302290, accuracy: 0.812400\n",
            "iter: 428, logloss: 0.302240, accuracy: 0.812400\n",
            "iter: 429, logloss: 0.302189, accuracy: 0.812400\n",
            "iter: 430, logloss: 0.302140, accuracy: 0.812200\n",
            "iter: 431, logloss: 0.302090, accuracy: 0.812200\n",
            "iter: 432, logloss: 0.302040, accuracy: 0.812400\n",
            "iter: 433, logloss: 0.301991, accuracy: 0.812200\n",
            "iter: 434, logloss: 0.301942, accuracy: 0.812200\n",
            "iter: 435, logloss: 0.301892, accuracy: 0.812400\n",
            "iter: 436, logloss: 0.301844, accuracy: 0.812400\n",
            "iter: 437, logloss: 0.301794, accuracy: 0.812400\n",
            "iter: 438, logloss: 0.301746, accuracy: 0.812400\n",
            "iter: 439, logloss: 0.301698, accuracy: 0.812400\n",
            "iter: 440, logloss: 0.301649, accuracy: 0.812800\n",
            "iter: 441, logloss: 0.301602, accuracy: 0.813200\n",
            "iter: 442, logloss: 0.301554, accuracy: 0.813400\n",
            "iter: 443, logloss: 0.301506, accuracy: 0.813400\n",
            "iter: 444, logloss: 0.301458, accuracy: 0.813200\n",
            "iter: 445, logloss: 0.301411, accuracy: 0.813600\n",
            "iter: 446, logloss: 0.301364, accuracy: 0.813600\n",
            "iter: 447, logloss: 0.301316, accuracy: 0.813600\n",
            "iter: 448, logloss: 0.301268, accuracy: 0.813600\n",
            "iter: 449, logloss: 0.301223, accuracy: 0.813600\n",
            "iter: 450, logloss: 0.301176, accuracy: 0.813600\n",
            "iter: 451, logloss: 0.301129, accuracy: 0.813800\n",
            "iter: 452, logloss: 0.301083, accuracy: 0.814000\n",
            "iter: 453, logloss: 0.301037, accuracy: 0.814200\n",
            "iter: 454, logloss: 0.300990, accuracy: 0.814400\n",
            "iter: 455, logloss: 0.300945, accuracy: 0.814200\n",
            "iter: 456, logloss: 0.300899, accuracy: 0.814800\n",
            "iter: 457, logloss: 0.300853, accuracy: 0.815000\n",
            "iter: 458, logloss: 0.300808, accuracy: 0.815000\n",
            "iter: 459, logloss: 0.300764, accuracy: 0.815200\n",
            "iter: 460, logloss: 0.300717, accuracy: 0.815200\n",
            "iter: 461, logloss: 0.300673, accuracy: 0.815200\n",
            "iter: 462, logloss: 0.300628, accuracy: 0.815600\n",
            "iter: 463, logloss: 0.300583, accuracy: 0.815600\n",
            "iter: 464, logloss: 0.300539, accuracy: 0.815600\n",
            "iter: 465, logloss: 0.300495, accuracy: 0.815800\n",
            "iter: 466, logloss: 0.300450, accuracy: 0.815600\n",
            "iter: 467, logloss: 0.300407, accuracy: 0.815600\n",
            "iter: 468, logloss: 0.300363, accuracy: 0.815600\n",
            "iter: 469, logloss: 0.300319, accuracy: 0.815600\n",
            "iter: 470, logloss: 0.300275, accuracy: 0.815600\n",
            "iter: 471, logloss: 0.300232, accuracy: 0.815600\n",
            "iter: 472, logloss: 0.300189, accuracy: 0.815600\n",
            "iter: 473, logloss: 0.300145, accuracy: 0.815600\n",
            "iter: 474, logloss: 0.300101, accuracy: 0.815800\n",
            "iter: 475, logloss: 0.300059, accuracy: 0.815800\n",
            "iter: 476, logloss: 0.300017, accuracy: 0.816000\n",
            "iter: 477, logloss: 0.299973, accuracy: 0.816000\n",
            "iter: 478, logloss: 0.299931, accuracy: 0.816200\n",
            "iter: 479, logloss: 0.299889, accuracy: 0.816200\n",
            "iter: 480, logloss: 0.299847, accuracy: 0.816400\n",
            "iter: 481, logloss: 0.299805, accuracy: 0.816600\n",
            "iter: 482, logloss: 0.299764, accuracy: 0.816800\n",
            "iter: 483, logloss: 0.299722, accuracy: 0.816600\n",
            "iter: 484, logloss: 0.299680, accuracy: 0.817000\n",
            "iter: 485, logloss: 0.299639, accuracy: 0.817200\n",
            "iter: 486, logloss: 0.299596, accuracy: 0.817200\n",
            "iter: 487, logloss: 0.299555, accuracy: 0.817400\n",
            "iter: 488, logloss: 0.299514, accuracy: 0.817200\n",
            "iter: 489, logloss: 0.299474, accuracy: 0.817200\n",
            "iter: 490, logloss: 0.299433, accuracy: 0.817200\n",
            "iter: 491, logloss: 0.299392, accuracy: 0.817400\n",
            "iter: 492, logloss: 0.299351, accuracy: 0.817800\n",
            "iter: 493, logloss: 0.299311, accuracy: 0.818000\n",
            "iter: 494, logloss: 0.299271, accuracy: 0.818000\n",
            "iter: 495, logloss: 0.299231, accuracy: 0.818400\n",
            "iter: 496, logloss: 0.299190, accuracy: 0.818600\n",
            "iter: 497, logloss: 0.299151, accuracy: 0.818800\n",
            "iter: 498, logloss: 0.299111, accuracy: 0.818800\n",
            "iter: 499, logloss: 0.299072, accuracy: 0.818800\n",
            "Duration (s): 143.752182\n",
            "final accuracy: 0.818800\n",
            "final weights: \n",
            "[\n",
            "-0.912856,\t1.054308;\n",
            "-0.200946,\t0.551628;\n",
            "-0.557171,\t0.623020;\n",
            "0.260683,\t-0.198299;\n",
            "-0.056367,\t-0.045094;\n",
            "0.354063,\t-0.289981;\n",
            "-0.693167,\t0.619710;\n",
            "0.636540,\t-0.112859;\n",
            "0.988858,\t-0.775986\n",
            "]\n",
            "[Thread 0x7f4f7d6d8700 (LWP 436) exited]\n",
            "[Thread 0x7f4f8c6053c0 (LWP 427) exited]\n",
            "[Inferior 1 (process 427) exited normally]\n",
            "tmp.txt:5: Error in sourced command file:\n",
            "No stack.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGJ6uVNBVHUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625ae367-1d75-4855-c0c7-88966ec9175d"
      },
      "source": [
        "!cuda-memcheck ./a.out "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "========= CUDA-MEMCHECK\n",
            "headers: \"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"!\n",
            "Read 17000 rows.\n",
            "Allocated memory for inputs: 17000 rows, 9 columns.\n",
            "Allocated memory for labels: 17000 rows, 2 columns.\n",
            "Inputs (first 10):\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "-114.31\t-114.47\t-114.56\t-114.57\t-114.57\t-114.58\t-114.58\t-114.59\t-114.59\t-114.6\t\n",
            "34.19\t34.4\t33.69\t33.64\t33.57\t33.63\t33.61\t34.83\t33.61\t34.83\t\n",
            "15\t19\t17\t14\t20\t29\t25\t41\t34\t46\t\n",
            "5612\t7650\t720\t1501\t1454\t1387\t2907\t812\t4789\t1497\t\n",
            "1283\t1901\t174\t337\t326\t236\t680\t168\t1175\t309\t\n",
            "1015\t1129\t333\t515\t624\t671\t1841\t375\t3134\t787\t\n",
            "472\t463\t117\t226\t262\t239\t633\t158\t1056\t271\t\n",
            "1.4936\t1.82\t1.6509\t3.1917\t1.925\t3.3438\t2.6768\t1.7083\t2.1782\t2.1908\t\n",
            "Labels (first 10):\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "1\t1\t1\t1\t1\t1\t1\t1\t1\t1\t\n",
            "initial accuracy: 0.340200\n",
            "iter: 0, logloss: 0.880316, accuracy: 0.340000\n",
            "iter: 1, logloss: 0.830006, accuracy: 0.340200\n",
            "iter: 2, logloss: 0.785198, accuracy: 0.342600\n",
            "iter: 3, logloss: 0.745490, accuracy: 0.370600\n",
            "iter: 4, logloss: 0.710397, accuracy: 0.434200\n",
            "iter: 5, logloss: 0.679411, accuracy: 0.529800\n",
            "iter: 6, logloss: 0.652040, accuracy: 0.622000\n",
            "iter: 7, logloss: 0.627811, accuracy: 0.691800\n",
            "iter: 8, logloss: 0.606301, accuracy: 0.730200\n",
            "iter: 9, logloss: 0.587134, accuracy: 0.743200\n",
            "iter: 10, logloss: 0.569993, accuracy: 0.743600\n",
            "iter: 11, logloss: 0.554603, accuracy: 0.743600\n",
            "iter: 12, logloss: 0.540726, accuracy: 0.739800\n",
            "iter: 13, logloss: 0.528171, accuracy: 0.735400\n",
            "iter: 14, logloss: 0.516767, accuracy: 0.733000\n",
            "iter: 15, logloss: 0.506375, accuracy: 0.730200\n",
            "iter: 16, logloss: 0.496874, accuracy: 0.728600\n",
            "iter: 17, logloss: 0.488162, accuracy: 0.727200\n",
            "iter: 18, logloss: 0.480151, accuracy: 0.725800\n",
            "iter: 19, logloss: 0.472766, accuracy: 0.724800\n",
            "iter: 20, logloss: 0.465936, accuracy: 0.723800\n",
            "iter: 21, logloss: 0.459610, accuracy: 0.723600\n",
            "iter: 22, logloss: 0.453735, accuracy: 0.723800\n",
            "iter: 23, logloss: 0.448267, accuracy: 0.723600\n",
            "iter: 24, logloss: 0.443169, accuracy: 0.723600\n",
            "iter: 25, logloss: 0.438407, accuracy: 0.723800\n",
            "iter: 26, logloss: 0.433951, accuracy: 0.723600\n",
            "iter: 27, logloss: 0.429771, accuracy: 0.723800\n",
            "iter: 28, logloss: 0.425848, accuracy: 0.724000\n",
            "iter: 29, logloss: 0.422157, accuracy: 0.724200\n",
            "iter: 30, logloss: 0.418679, accuracy: 0.724400\n",
            "iter: 31, logloss: 0.415401, accuracy: 0.724600\n",
            "iter: 32, logloss: 0.412301, accuracy: 0.724800\n",
            "iter: 33, logloss: 0.409372, accuracy: 0.724800\n",
            "iter: 34, logloss: 0.406597, accuracy: 0.725000\n",
            "iter: 35, logloss: 0.403964, accuracy: 0.725400\n",
            "iter: 36, logloss: 0.401464, accuracy: 0.726400\n",
            "iter: 37, logloss: 0.399088, accuracy: 0.727000\n",
            "iter: 38, logloss: 0.396828, accuracy: 0.727400\n",
            "iter: 39, logloss: 0.394675, accuracy: 0.727800\n",
            "iter: 40, logloss: 0.392620, accuracy: 0.727800\n",
            "iter: 41, logloss: 0.390660, accuracy: 0.728800\n",
            "iter: 42, logloss: 0.388785, accuracy: 0.729400\n",
            "iter: 43, logloss: 0.386994, accuracy: 0.730000\n",
            "iter: 44, logloss: 0.385277, accuracy: 0.730000\n",
            "iter: 45, logloss: 0.383634, accuracy: 0.730400\n",
            "iter: 46, logloss: 0.382058, accuracy: 0.730600\n",
            "iter: 47, logloss: 0.380545, accuracy: 0.730600\n",
            "iter: 48, logloss: 0.379091, accuracy: 0.731600\n",
            "iter: 49, logloss: 0.377695, accuracy: 0.732200\n",
            "iter: 50, logloss: 0.376349, accuracy: 0.733000\n",
            "iter: 51, logloss: 0.375054, accuracy: 0.733200\n",
            "iter: 52, logloss: 0.373807, accuracy: 0.733600\n",
            "iter: 53, logloss: 0.372606, accuracy: 0.733800\n",
            "iter: 54, logloss: 0.371445, accuracy: 0.734200\n",
            "iter: 55, logloss: 0.370325, accuracy: 0.734600\n",
            "iter: 56, logloss: 0.369244, accuracy: 0.735400\n",
            "iter: 57, logloss: 0.368197, accuracy: 0.736200\n",
            "iter: 58, logloss: 0.367185, accuracy: 0.736800\n",
            "iter: 59, logloss: 0.366205, accuracy: 0.737400\n",
            "iter: 60, logloss: 0.365257, accuracy: 0.737800\n",
            "iter: 61, logloss: 0.364338, accuracy: 0.738400\n",
            "iter: 62, logloss: 0.363446, accuracy: 0.738800\n",
            "iter: 63, logloss: 0.362582, accuracy: 0.738800\n",
            "iter: 64, logloss: 0.361742, accuracy: 0.739200\n",
            "iter: 65, logloss: 0.360929, accuracy: 0.739200\n",
            "iter: 66, logloss: 0.360137, accuracy: 0.740400\n",
            "iter: 67, logloss: 0.359368, accuracy: 0.740800\n",
            "iter: 68, logloss: 0.358621, accuracy: 0.741400\n",
            "iter: 69, logloss: 0.357892, accuracy: 0.742000\n",
            "iter: 70, logloss: 0.357182, accuracy: 0.742400\n",
            "iter: 71, logloss: 0.356493, accuracy: 0.742800\n",
            "iter: 72, logloss: 0.355821, accuracy: 0.743800\n",
            "iter: 73, logloss: 0.355166, accuracy: 0.744200\n",
            "iter: 74, logloss: 0.354528, accuracy: 0.744600\n",
            "iter: 75, logloss: 0.353904, accuracy: 0.744800\n",
            "iter: 76, logloss: 0.353296, accuracy: 0.745400\n",
            "iter: 77, logloss: 0.352704, accuracy: 0.745600\n",
            "iter: 78, logloss: 0.352125, accuracy: 0.746000\n",
            "iter: 79, logloss: 0.351558, accuracy: 0.746200\n",
            "iter: 80, logloss: 0.351005, accuracy: 0.746400\n",
            "iter: 81, logloss: 0.350463, accuracy: 0.746800\n",
            "iter: 82, logloss: 0.349935, accuracy: 0.747200\n",
            "iter: 83, logloss: 0.349419, accuracy: 0.747200\n",
            "iter: 84, logloss: 0.348912, accuracy: 0.747400\n",
            "iter: 85, logloss: 0.348416, accuracy: 0.747400\n",
            "iter: 86, logloss: 0.347931, accuracy: 0.747600\n",
            "iter: 87, logloss: 0.347456, accuracy: 0.747600\n",
            "iter: 88, logloss: 0.346990, accuracy: 0.748200\n",
            "iter: 89, logloss: 0.346533, accuracy: 0.748800\n",
            "iter: 90, logloss: 0.346086, accuracy: 0.749200\n",
            "iter: 91, logloss: 0.345648, accuracy: 0.749200\n",
            "iter: 92, logloss: 0.345217, accuracy: 0.749800\n",
            "iter: 93, logloss: 0.344795, accuracy: 0.750400\n",
            "iter: 94, logloss: 0.344381, accuracy: 0.750600\n",
            "iter: 95, logloss: 0.343974, accuracy: 0.751000\n",
            "iter: 96, logloss: 0.343573, accuracy: 0.751200\n",
            "iter: 97, logloss: 0.343181, accuracy: 0.751400\n",
            "iter: 98, logloss: 0.342796, accuracy: 0.751400\n",
            "iter: 99, logloss: 0.342417, accuracy: 0.751600\n",
            "iter: 100, logloss: 0.342044, accuracy: 0.752000\n",
            "iter: 101, logloss: 0.341678, accuracy: 0.752800\n",
            "iter: 102, logloss: 0.341319, accuracy: 0.752800\n",
            "iter: 103, logloss: 0.340965, accuracy: 0.753000\n",
            "iter: 104, logloss: 0.340616, accuracy: 0.753600\n",
            "iter: 105, logloss: 0.340273, accuracy: 0.753800\n",
            "iter: 106, logloss: 0.339935, accuracy: 0.754200\n",
            "iter: 107, logloss: 0.339603, accuracy: 0.755000\n",
            "iter: 108, logloss: 0.339276, accuracy: 0.755000\n",
            "iter: 109, logloss: 0.338955, accuracy: 0.755400\n",
            "iter: 110, logloss: 0.338637, accuracy: 0.755800\n",
            "iter: 111, logloss: 0.338325, accuracy: 0.756000\n",
            "iter: 112, logloss: 0.338018, accuracy: 0.756200\n",
            "iter: 113, logloss: 0.337713, accuracy: 0.756800\n",
            "iter: 114, logloss: 0.337415, accuracy: 0.757600\n",
            "iter: 115, logloss: 0.337120, accuracy: 0.757600\n",
            "iter: 116, logloss: 0.336828, accuracy: 0.757800\n",
            "iter: 117, logloss: 0.336541, accuracy: 0.758600\n",
            "iter: 118, logloss: 0.336259, accuracy: 0.758600\n",
            "iter: 119, logloss: 0.335979, accuracy: 0.759200\n",
            "iter: 120, logloss: 0.335703, accuracy: 0.759400\n",
            "iter: 121, logloss: 0.335433, accuracy: 0.759600\n",
            "iter: 122, logloss: 0.335164, accuracy: 0.760200\n",
            "iter: 123, logloss: 0.334898, accuracy: 0.760600\n",
            "iter: 124, logloss: 0.334637, accuracy: 0.760600\n",
            "iter: 125, logloss: 0.334379, accuracy: 0.760600\n",
            "iter: 126, logloss: 0.334123, accuracy: 0.761000\n",
            "iter: 127, logloss: 0.333872, accuracy: 0.761600\n",
            "iter: 128, logloss: 0.333622, accuracy: 0.761800\n",
            "iter: 129, logloss: 0.333377, accuracy: 0.762200\n",
            "iter: 130, logloss: 0.333133, accuracy: 0.762400\n",
            "iter: 131, logloss: 0.332893, accuracy: 0.762800\n",
            "iter: 132, logloss: 0.332656, accuracy: 0.763400\n",
            "iter: 133, logloss: 0.332421, accuracy: 0.764200\n",
            "iter: 134, logloss: 0.332188, accuracy: 0.764400\n",
            "iter: 135, logloss: 0.331958, accuracy: 0.764600\n",
            "iter: 136, logloss: 0.331732, accuracy: 0.764800\n",
            "iter: 137, logloss: 0.331507, accuracy: 0.765000\n",
            "iter: 138, logloss: 0.331284, accuracy: 0.765200\n",
            "iter: 139, logloss: 0.331065, accuracy: 0.765400\n",
            "iter: 140, logloss: 0.330847, accuracy: 0.765400\n",
            "iter: 141, logloss: 0.330632, accuracy: 0.765800\n",
            "iter: 142, logloss: 0.330418, accuracy: 0.766200\n",
            "iter: 143, logloss: 0.330208, accuracy: 0.766600\n",
            "iter: 144, logloss: 0.330000, accuracy: 0.767400\n",
            "iter: 145, logloss: 0.329793, accuracy: 0.767600\n",
            "iter: 146, logloss: 0.329589, accuracy: 0.767800\n",
            "iter: 147, logloss: 0.329386, accuracy: 0.768200\n",
            "iter: 148, logloss: 0.329185, accuracy: 0.768400\n",
            "iter: 149, logloss: 0.328987, accuracy: 0.768800\n",
            "iter: 150, logloss: 0.328789, accuracy: 0.769000\n",
            "iter: 151, logloss: 0.328595, accuracy: 0.769200\n",
            "iter: 152, logloss: 0.328400, accuracy: 0.769200\n",
            "iter: 153, logloss: 0.328210, accuracy: 0.769600\n",
            "iter: 154, logloss: 0.328020, accuracy: 0.769600\n",
            "iter: 155, logloss: 0.327833, accuracy: 0.770000\n",
            "iter: 156, logloss: 0.327646, accuracy: 0.770200\n",
            "iter: 157, logloss: 0.327462, accuracy: 0.770600\n",
            "iter: 158, logloss: 0.327280, accuracy: 0.770800\n",
            "iter: 159, logloss: 0.327099, accuracy: 0.770800\n",
            "iter: 160, logloss: 0.326919, accuracy: 0.771000\n",
            "iter: 161, logloss: 0.326740, accuracy: 0.771200\n",
            "iter: 162, logloss: 0.326564, accuracy: 0.772000\n",
            "iter: 163, logloss: 0.326388, accuracy: 0.772800\n",
            "iter: 164, logloss: 0.326216, accuracy: 0.773200\n",
            "iter: 165, logloss: 0.326042, accuracy: 0.773600\n",
            "iter: 166, logloss: 0.325871, accuracy: 0.773800\n",
            "iter: 167, logloss: 0.325702, accuracy: 0.774200\n",
            "iter: 168, logloss: 0.325534, accuracy: 0.774200\n",
            "iter: 169, logloss: 0.325367, accuracy: 0.774400\n",
            "iter: 170, logloss: 0.325202, accuracy: 0.774800\n",
            "iter: 171, logloss: 0.325038, accuracy: 0.774800\n",
            "iter: 172, logloss: 0.324875, accuracy: 0.775000\n",
            "iter: 173, logloss: 0.324713, accuracy: 0.775000\n",
            "iter: 174, logloss: 0.324553, accuracy: 0.775000\n",
            "iter: 175, logloss: 0.324394, accuracy: 0.775400\n",
            "iter: 176, logloss: 0.324236, accuracy: 0.775600\n",
            "iter: 177, logloss: 0.324079, accuracy: 0.775800\n",
            "iter: 178, logloss: 0.323924, accuracy: 0.775600\n",
            "iter: 179, logloss: 0.323769, accuracy: 0.775600\n",
            "iter: 180, logloss: 0.323615, accuracy: 0.776400\n",
            "iter: 181, logloss: 0.323464, accuracy: 0.776600\n",
            "iter: 182, logloss: 0.323312, accuracy: 0.776600\n",
            "iter: 183, logloss: 0.323162, accuracy: 0.776800\n",
            "iter: 184, logloss: 0.323013, accuracy: 0.777200\n",
            "iter: 185, logloss: 0.322866, accuracy: 0.777800\n",
            "iter: 186, logloss: 0.322718, accuracy: 0.777600\n",
            "iter: 187, logloss: 0.322573, accuracy: 0.778000\n",
            "iter: 188, logloss: 0.322428, accuracy: 0.779000\n",
            "iter: 189, logloss: 0.322284, accuracy: 0.779200\n",
            "iter: 190, logloss: 0.322141, accuracy: 0.780200\n",
            "iter: 191, logloss: 0.321998, accuracy: 0.780400\n",
            "iter: 192, logloss: 0.321858, accuracy: 0.780400\n",
            "iter: 193, logloss: 0.321718, accuracy: 0.780400\n",
            "iter: 194, logloss: 0.321579, accuracy: 0.780600\n",
            "iter: 195, logloss: 0.321440, accuracy: 0.780600\n",
            "iter: 196, logloss: 0.321303, accuracy: 0.780600\n",
            "iter: 197, logloss: 0.321167, accuracy: 0.780800\n",
            "iter: 198, logloss: 0.321031, accuracy: 0.780800\n",
            "iter: 199, logloss: 0.320896, accuracy: 0.781200\n",
            "iter: 200, logloss: 0.320762, accuracy: 0.781200\n",
            "iter: 201, logloss: 0.320630, accuracy: 0.781200\n",
            "iter: 202, logloss: 0.320497, accuracy: 0.781200\n",
            "iter: 203, logloss: 0.320365, accuracy: 0.781200\n",
            "iter: 204, logloss: 0.320235, accuracy: 0.781600\n",
            "iter: 205, logloss: 0.320106, accuracy: 0.781800\n",
            "iter: 206, logloss: 0.319976, accuracy: 0.781800\n",
            "iter: 207, logloss: 0.319848, accuracy: 0.782200\n",
            "iter: 208, logloss: 0.319721, accuracy: 0.782400\n",
            "iter: 209, logloss: 0.319593, accuracy: 0.782800\n",
            "iter: 210, logloss: 0.319467, accuracy: 0.783400\n",
            "iter: 211, logloss: 0.319343, accuracy: 0.783600\n",
            "iter: 212, logloss: 0.319218, accuracy: 0.783800\n",
            "iter: 213, logloss: 0.319094, accuracy: 0.784400\n",
            "iter: 214, logloss: 0.318971, accuracy: 0.784400\n",
            "iter: 215, logloss: 0.318848, accuracy: 0.784600\n",
            "iter: 216, logloss: 0.318727, accuracy: 0.785200\n",
            "iter: 217, logloss: 0.318607, accuracy: 0.785800\n",
            "iter: 218, logloss: 0.318486, accuracy: 0.785800\n",
            "iter: 219, logloss: 0.318367, accuracy: 0.786000\n",
            "iter: 220, logloss: 0.318248, accuracy: 0.785800\n",
            "iter: 221, logloss: 0.318129, accuracy: 0.786000\n",
            "iter: 222, logloss: 0.318011, accuracy: 0.786400\n",
            "iter: 223, logloss: 0.317895, accuracy: 0.787000\n",
            "iter: 224, logloss: 0.317779, accuracy: 0.787400\n",
            "iter: 225, logloss: 0.317662, accuracy: 0.787800\n",
            "iter: 226, logloss: 0.317548, accuracy: 0.787800\n",
            "iter: 227, logloss: 0.317433, accuracy: 0.787800\n",
            "iter: 228, logloss: 0.317319, accuracy: 0.787800\n",
            "iter: 229, logloss: 0.317206, accuracy: 0.788200\n",
            "iter: 230, logloss: 0.317094, accuracy: 0.788400\n",
            "iter: 231, logloss: 0.316982, accuracy: 0.788400\n",
            "iter: 232, logloss: 0.316870, accuracy: 0.788400\n",
            "iter: 233, logloss: 0.316759, accuracy: 0.788400\n",
            "iter: 234, logloss: 0.316649, accuracy: 0.788600\n",
            "iter: 235, logloss: 0.316540, accuracy: 0.788600\n",
            "iter: 236, logloss: 0.316431, accuracy: 0.789000\n",
            "iter: 237, logloss: 0.316322, accuracy: 0.789400\n",
            "iter: 238, logloss: 0.316214, accuracy: 0.790000\n",
            "iter: 239, logloss: 0.316107, accuracy: 0.790200\n",
            "iter: 240, logloss: 0.316000, accuracy: 0.790400\n",
            "iter: 241, logloss: 0.315893, accuracy: 0.790400\n",
            "iter: 242, logloss: 0.315788, accuracy: 0.790600\n",
            "iter: 243, logloss: 0.315682, accuracy: 0.790800\n",
            "iter: 244, logloss: 0.315577, accuracy: 0.790800\n",
            "iter: 245, logloss: 0.315473, accuracy: 0.791400\n",
            "iter: 246, logloss: 0.315370, accuracy: 0.791600\n",
            "iter: 247, logloss: 0.315266, accuracy: 0.791600\n",
            "iter: 248, logloss: 0.315163, accuracy: 0.791600\n",
            "iter: 249, logloss: 0.315061, accuracy: 0.792000\n",
            "iter: 250, logloss: 0.314960, accuracy: 0.792000\n",
            "iter: 251, logloss: 0.314859, accuracy: 0.792000\n",
            "iter: 252, logloss: 0.314758, accuracy: 0.792000\n",
            "iter: 253, logloss: 0.314657, accuracy: 0.792200\n",
            "iter: 254, logloss: 0.314558, accuracy: 0.792200\n",
            "iter: 255, logloss: 0.314459, accuracy: 0.792200\n",
            "iter: 256, logloss: 0.314360, accuracy: 0.792400\n",
            "iter: 257, logloss: 0.314261, accuracy: 0.792400\n",
            "iter: 258, logloss: 0.314164, accuracy: 0.792600\n",
            "iter: 259, logloss: 0.314066, accuracy: 0.792800\n",
            "iter: 260, logloss: 0.313969, accuracy: 0.792600\n",
            "iter: 261, logloss: 0.313873, accuracy: 0.793000\n",
            "iter: 262, logloss: 0.313776, accuracy: 0.793400\n",
            "iter: 263, logloss: 0.313680, accuracy: 0.793600\n",
            "iter: 264, logloss: 0.313585, accuracy: 0.794000\n",
            "iter: 265, logloss: 0.313491, accuracy: 0.794200\n",
            "iter: 266, logloss: 0.313396, accuracy: 0.794400\n",
            "iter: 267, logloss: 0.313302, accuracy: 0.794600\n",
            "iter: 268, logloss: 0.313210, accuracy: 0.794600\n",
            "iter: 269, logloss: 0.313116, accuracy: 0.794800\n",
            "iter: 270, logloss: 0.313024, accuracy: 0.794600\n",
            "iter: 271, logloss: 0.312932, accuracy: 0.794600\n",
            "iter: 272, logloss: 0.312840, accuracy: 0.794400\n",
            "iter: 273, logloss: 0.312749, accuracy: 0.794400\n",
            "iter: 274, logloss: 0.312657, accuracy: 0.794600\n",
            "iter: 275, logloss: 0.312567, accuracy: 0.794800\n",
            "iter: 276, logloss: 0.312477, accuracy: 0.794600\n",
            "iter: 277, logloss: 0.312388, accuracy: 0.794400\n",
            "iter: 278, logloss: 0.312298, accuracy: 0.794800\n",
            "iter: 279, logloss: 0.312209, accuracy: 0.794800\n",
            "iter: 280, logloss: 0.312120, accuracy: 0.795000\n",
            "iter: 281, logloss: 0.312032, accuracy: 0.795400\n",
            "iter: 282, logloss: 0.311944, accuracy: 0.795600\n",
            "iter: 283, logloss: 0.311857, accuracy: 0.795800\n",
            "iter: 284, logloss: 0.311769, accuracy: 0.796000\n",
            "iter: 285, logloss: 0.311683, accuracy: 0.796200\n",
            "iter: 286, logloss: 0.311597, accuracy: 0.796400\n",
            "iter: 287, logloss: 0.311511, accuracy: 0.796600\n",
            "iter: 288, logloss: 0.311425, accuracy: 0.796800\n",
            "iter: 289, logloss: 0.311340, accuracy: 0.796800\n",
            "iter: 290, logloss: 0.311256, accuracy: 0.796800\n",
            "iter: 291, logloss: 0.311172, accuracy: 0.797200\n",
            "iter: 292, logloss: 0.311087, accuracy: 0.797200\n",
            "iter: 293, logloss: 0.311003, accuracy: 0.797400\n",
            "iter: 294, logloss: 0.310920, accuracy: 0.797600\n",
            "iter: 295, logloss: 0.310837, accuracy: 0.797600\n",
            "iter: 296, logloss: 0.310754, accuracy: 0.797600\n",
            "iter: 297, logloss: 0.310673, accuracy: 0.798000\n",
            "iter: 298, logloss: 0.310590, accuracy: 0.798200\n",
            "iter: 299, logloss: 0.310509, accuracy: 0.798600\n",
            "iter: 300, logloss: 0.310428, accuracy: 0.798600\n",
            "iter: 301, logloss: 0.310346, accuracy: 0.798800\n",
            "iter: 302, logloss: 0.310265, accuracy: 0.798800\n",
            "iter: 303, logloss: 0.310186, accuracy: 0.799000\n",
            "iter: 304, logloss: 0.310105, accuracy: 0.799200\n",
            "iter: 305, logloss: 0.310026, accuracy: 0.799600\n",
            "iter: 306, logloss: 0.309947, accuracy: 0.799600\n",
            "iter: 307, logloss: 0.309867, accuracy: 0.799800\n",
            "iter: 308, logloss: 0.309789, accuracy: 0.800000\n",
            "iter: 309, logloss: 0.309711, accuracy: 0.800200\n",
            "iter: 310, logloss: 0.309632, accuracy: 0.800800\n",
            "iter: 311, logloss: 0.309555, accuracy: 0.800800\n",
            "iter: 312, logloss: 0.309478, accuracy: 0.801000\n",
            "iter: 313, logloss: 0.309401, accuracy: 0.801200\n",
            "iter: 314, logloss: 0.309323, accuracy: 0.801600\n",
            "iter: 315, logloss: 0.309247, accuracy: 0.801600\n",
            "iter: 316, logloss: 0.309171, accuracy: 0.801600\n",
            "iter: 317, logloss: 0.309095, accuracy: 0.801800\n",
            "iter: 318, logloss: 0.309019, accuracy: 0.802000\n",
            "iter: 319, logloss: 0.308944, accuracy: 0.802000\n",
            "iter: 320, logloss: 0.308870, accuracy: 0.802000\n",
            "iter: 321, logloss: 0.308795, accuracy: 0.802000\n",
            "iter: 322, logloss: 0.308721, accuracy: 0.802200\n",
            "iter: 323, logloss: 0.308647, accuracy: 0.802400\n",
            "iter: 324, logloss: 0.308573, accuracy: 0.802800\n",
            "iter: 325, logloss: 0.308499, accuracy: 0.802800\n",
            "iter: 326, logloss: 0.308426, accuracy: 0.803000\n",
            "iter: 327, logloss: 0.308354, accuracy: 0.803000\n",
            "iter: 328, logloss: 0.308281, accuracy: 0.803200\n",
            "iter: 329, logloss: 0.308210, accuracy: 0.803600\n",
            "iter: 330, logloss: 0.308136, accuracy: 0.803600\n",
            "iter: 331, logloss: 0.308065, accuracy: 0.803800\n",
            "iter: 332, logloss: 0.307993, accuracy: 0.803800\n",
            "iter: 333, logloss: 0.307921, accuracy: 0.803600\n",
            "iter: 334, logloss: 0.307851, accuracy: 0.803800\n",
            "iter: 335, logloss: 0.307780, accuracy: 0.803800\n",
            "iter: 336, logloss: 0.307711, accuracy: 0.804200\n",
            "iter: 337, logloss: 0.307640, accuracy: 0.804200\n",
            "iter: 338, logloss: 0.307571, accuracy: 0.804200\n",
            "iter: 339, logloss: 0.307502, accuracy: 0.804600\n",
            "iter: 340, logloss: 0.307431, accuracy: 0.804800\n",
            "iter: 341, logloss: 0.307362, accuracy: 0.804800\n",
            "iter: 342, logloss: 0.307294, accuracy: 0.804800\n",
            "iter: 343, logloss: 0.307226, accuracy: 0.804800\n",
            "iter: 344, logloss: 0.307157, accuracy: 0.804800\n",
            "iter: 345, logloss: 0.307090, accuracy: 0.805000\n",
            "iter: 346, logloss: 0.307021, accuracy: 0.805200\n",
            "iter: 347, logloss: 0.306953, accuracy: 0.805200\n",
            "iter: 348, logloss: 0.306887, accuracy: 0.805600\n",
            "iter: 349, logloss: 0.306820, accuracy: 0.805800\n",
            "iter: 350, logloss: 0.306753, accuracy: 0.805800\n",
            "iter: 351, logloss: 0.306687, accuracy: 0.806000\n",
            "iter: 352, logloss: 0.306622, accuracy: 0.806000\n",
            "iter: 353, logloss: 0.306556, accuracy: 0.805800\n",
            "iter: 354, logloss: 0.306490, accuracy: 0.806000\n",
            "iter: 355, logloss: 0.306425, accuracy: 0.806000\n",
            "iter: 356, logloss: 0.306359, accuracy: 0.805800\n",
            "iter: 357, logloss: 0.306293, accuracy: 0.806000\n",
            "iter: 358, logloss: 0.306229, accuracy: 0.806200\n",
            "iter: 359, logloss: 0.306165, accuracy: 0.806200\n",
            "iter: 360, logloss: 0.306100, accuracy: 0.806200\n",
            "iter: 361, logloss: 0.306037, accuracy: 0.806200\n",
            "iter: 362, logloss: 0.305973, accuracy: 0.806200\n",
            "iter: 363, logloss: 0.305909, accuracy: 0.806400\n",
            "iter: 364, logloss: 0.305846, accuracy: 0.806600\n",
            "iter: 365, logloss: 0.305783, accuracy: 0.806600\n",
            "iter: 366, logloss: 0.305720, accuracy: 0.806800\n",
            "iter: 367, logloss: 0.305657, accuracy: 0.806800\n",
            "iter: 368, logloss: 0.305596, accuracy: 0.807000\n",
            "iter: 369, logloss: 0.305533, accuracy: 0.807000\n",
            "iter: 370, logloss: 0.305472, accuracy: 0.806800\n",
            "iter: 371, logloss: 0.305410, accuracy: 0.806800\n",
            "iter: 372, logloss: 0.305349, accuracy: 0.806800\n",
            "iter: 373, logloss: 0.305288, accuracy: 0.806800\n",
            "iter: 374, logloss: 0.305226, accuracy: 0.807000\n",
            "iter: 375, logloss: 0.305166, accuracy: 0.807200\n",
            "iter: 376, logloss: 0.305105, accuracy: 0.807200\n",
            "iter: 377, logloss: 0.305044, accuracy: 0.807400\n",
            "iter: 378, logloss: 0.304984, accuracy: 0.807200\n",
            "iter: 379, logloss: 0.304924, accuracy: 0.807200\n",
            "iter: 380, logloss: 0.304865, accuracy: 0.807200\n",
            "iter: 381, logloss: 0.304806, accuracy: 0.807200\n",
            "iter: 382, logloss: 0.304747, accuracy: 0.807400\n",
            "iter: 383, logloss: 0.304688, accuracy: 0.807600\n",
            "iter: 384, logloss: 0.304629, accuracy: 0.807600\n",
            "iter: 385, logloss: 0.304570, accuracy: 0.807800\n",
            "iter: 386, logloss: 0.304511, accuracy: 0.807800\n",
            "iter: 387, logloss: 0.304454, accuracy: 0.807800\n",
            "iter: 388, logloss: 0.304396, accuracy: 0.807600\n",
            "iter: 389, logloss: 0.304338, accuracy: 0.807600\n",
            "iter: 390, logloss: 0.304280, accuracy: 0.807600\n",
            "iter: 391, logloss: 0.304224, accuracy: 0.807600\n",
            "iter: 392, logloss: 0.304166, accuracy: 0.808000\n",
            "iter: 393, logloss: 0.304110, accuracy: 0.807800\n",
            "iter: 394, logloss: 0.304053, accuracy: 0.808200\n",
            "iter: 395, logloss: 0.303996, accuracy: 0.808200\n",
            "iter: 396, logloss: 0.303939, accuracy: 0.808400\n",
            "iter: 397, logloss: 0.303884, accuracy: 0.808800\n",
            "iter: 398, logloss: 0.303828, accuracy: 0.809000\n",
            "iter: 399, logloss: 0.303772, accuracy: 0.809000\n",
            "iter: 400, logloss: 0.303716, accuracy: 0.809200\n",
            "iter: 401, logloss: 0.303661, accuracy: 0.809200\n",
            "iter: 402, logloss: 0.303606, accuracy: 0.809200\n",
            "iter: 403, logloss: 0.303551, accuracy: 0.809200\n",
            "iter: 404, logloss: 0.303497, accuracy: 0.809600\n",
            "iter: 405, logloss: 0.303442, accuracy: 0.809800\n",
            "iter: 406, logloss: 0.303388, accuracy: 0.809800\n",
            "iter: 407, logloss: 0.303333, accuracy: 0.810000\n",
            "iter: 408, logloss: 0.303280, accuracy: 0.810000\n",
            "iter: 409, logloss: 0.303226, accuracy: 0.810000\n",
            "iter: 410, logloss: 0.303172, accuracy: 0.810200\n",
            "iter: 411, logloss: 0.303119, accuracy: 0.810200\n",
            "iter: 412, logloss: 0.303066, accuracy: 0.810200\n",
            "iter: 413, logloss: 0.303013, accuracy: 0.810400\n",
            "iter: 414, logloss: 0.302960, accuracy: 0.810400\n",
            "iter: 415, logloss: 0.302908, accuracy: 0.810400\n",
            "iter: 416, logloss: 0.302856, accuracy: 0.810400\n",
            "iter: 417, logloss: 0.302803, accuracy: 0.811000\n",
            "iter: 418, logloss: 0.302751, accuracy: 0.811200\n",
            "iter: 419, logloss: 0.302699, accuracy: 0.811400\n",
            "iter: 420, logloss: 0.302646, accuracy: 0.811800\n",
            "iter: 421, logloss: 0.302595, accuracy: 0.811600\n",
            "iter: 422, logloss: 0.302544, accuracy: 0.811600\n",
            "iter: 423, logloss: 0.302493, accuracy: 0.811600\n",
            "iter: 424, logloss: 0.302442, accuracy: 0.812000\n",
            "iter: 425, logloss: 0.302391, accuracy: 0.812200\n",
            "iter: 426, logloss: 0.302341, accuracy: 0.812400\n",
            "iter: 427, logloss: 0.302290, accuracy: 0.812400\n",
            "iter: 428, logloss: 0.302240, accuracy: 0.812400\n",
            "iter: 429, logloss: 0.302189, accuracy: 0.812400\n",
            "iter: 430, logloss: 0.302140, accuracy: 0.812200\n",
            "iter: 431, logloss: 0.302090, accuracy: 0.812200\n",
            "iter: 432, logloss: 0.302040, accuracy: 0.812400\n",
            "iter: 433, logloss: 0.301991, accuracy: 0.812200\n",
            "iter: 434, logloss: 0.301942, accuracy: 0.812200\n",
            "iter: 435, logloss: 0.301892, accuracy: 0.812400\n",
            "iter: 436, logloss: 0.301844, accuracy: 0.812400\n",
            "iter: 437, logloss: 0.301794, accuracy: 0.812400\n",
            "iter: 438, logloss: 0.301746, accuracy: 0.812400\n",
            "iter: 439, logloss: 0.301698, accuracy: 0.812400\n",
            "iter: 440, logloss: 0.301649, accuracy: 0.812800\n",
            "iter: 441, logloss: 0.301602, accuracy: 0.813200\n",
            "iter: 442, logloss: 0.301554, accuracy: 0.813400\n",
            "iter: 443, logloss: 0.301506, accuracy: 0.813400\n",
            "iter: 444, logloss: 0.301458, accuracy: 0.813200\n",
            "iter: 445, logloss: 0.301411, accuracy: 0.813600\n",
            "iter: 446, logloss: 0.301364, accuracy: 0.813600\n",
            "iter: 447, logloss: 0.301316, accuracy: 0.813600\n",
            "iter: 448, logloss: 0.301268, accuracy: 0.813600\n",
            "iter: 449, logloss: 0.301223, accuracy: 0.813600\n",
            "iter: 450, logloss: 0.301176, accuracy: 0.813600\n",
            "iter: 451, logloss: 0.301129, accuracy: 0.813800\n",
            "iter: 452, logloss: 0.301083, accuracy: 0.814000\n",
            "iter: 453, logloss: 0.301037, accuracy: 0.814200\n",
            "iter: 454, logloss: 0.300990, accuracy: 0.814400\n",
            "iter: 455, logloss: 0.300945, accuracy: 0.814200\n",
            "iter: 456, logloss: 0.300899, accuracy: 0.814800\n",
            "iter: 457, logloss: 0.300853, accuracy: 0.815000\n",
            "iter: 458, logloss: 0.300808, accuracy: 0.815000\n",
            "iter: 459, logloss: 0.300764, accuracy: 0.815200\n",
            "iter: 460, logloss: 0.300717, accuracy: 0.815200\n",
            "iter: 461, logloss: 0.300673, accuracy: 0.815200\n",
            "iter: 462, logloss: 0.300628, accuracy: 0.815600\n",
            "iter: 463, logloss: 0.300583, accuracy: 0.815600\n",
            "iter: 464, logloss: 0.300539, accuracy: 0.815600\n",
            "iter: 465, logloss: 0.300495, accuracy: 0.815800\n",
            "iter: 466, logloss: 0.300450, accuracy: 0.815600\n",
            "iter: 467, logloss: 0.300407, accuracy: 0.815600\n",
            "iter: 468, logloss: 0.300363, accuracy: 0.815600\n",
            "iter: 469, logloss: 0.300319, accuracy: 0.815600\n",
            "iter: 470, logloss: 0.300275, accuracy: 0.815600\n",
            "iter: 471, logloss: 0.300232, accuracy: 0.815600\n",
            "iter: 472, logloss: 0.300189, accuracy: 0.815600\n",
            "iter: 473, logloss: 0.300145, accuracy: 0.815600\n",
            "iter: 474, logloss: 0.300101, accuracy: 0.815800\n",
            "iter: 475, logloss: 0.300059, accuracy: 0.815800\n",
            "iter: 476, logloss: 0.300017, accuracy: 0.816000\n",
            "iter: 477, logloss: 0.299973, accuracy: 0.816000\n",
            "iter: 478, logloss: 0.299931, accuracy: 0.816200\n",
            "iter: 479, logloss: 0.299889, accuracy: 0.816200\n",
            "iter: 480, logloss: 0.299847, accuracy: 0.816400\n",
            "iter: 481, logloss: 0.299805, accuracy: 0.816600\n",
            "iter: 482, logloss: 0.299764, accuracy: 0.816800\n",
            "iter: 483, logloss: 0.299722, accuracy: 0.816600\n",
            "iter: 484, logloss: 0.299680, accuracy: 0.817000\n",
            "iter: 485, logloss: 0.299639, accuracy: 0.817200\n",
            "iter: 486, logloss: 0.299596, accuracy: 0.817200\n",
            "iter: 487, logloss: 0.299555, accuracy: 0.817400\n",
            "iter: 488, logloss: 0.299514, accuracy: 0.817200\n",
            "iter: 489, logloss: 0.299474, accuracy: 0.817200\n",
            "iter: 490, logloss: 0.299433, accuracy: 0.817200\n",
            "iter: 491, logloss: 0.299392, accuracy: 0.817400\n",
            "iter: 492, logloss: 0.299351, accuracy: 0.817800\n",
            "iter: 493, logloss: 0.299311, accuracy: 0.818000\n",
            "iter: 494, logloss: 0.299271, accuracy: 0.818000\n",
            "iter: 495, logloss: 0.299231, accuracy: 0.818400\n",
            "iter: 496, logloss: 0.299190, accuracy: 0.818600\n",
            "iter: 497, logloss: 0.299151, accuracy: 0.818800\n",
            "iter: 498, logloss: 0.299111, accuracy: 0.818800\n",
            "iter: 499, logloss: 0.299072, accuracy: 0.818800\n",
            "Duration (s): 20.661461\n",
            "final accuracy: 0.818800\n",
            "final weights: \n",
            "[\n",
            "-0.912856,\t1.054308;\n",
            "-0.200946,\t0.551628;\n",
            "-0.557171,\t0.623020;\n",
            "0.260683,\t-0.198299;\n",
            "-0.056367,\t-0.045094;\n",
            "0.354063,\t-0.289981;\n",
            "-0.693167,\t0.619710;\n",
            "0.636540,\t-0.112859;\n",
            "0.988858,\t-0.775986\n",
            "]\n",
            "========= ERROR SUMMARY: 0 errors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Pk-JL7rAhi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqs64V1tEysf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}